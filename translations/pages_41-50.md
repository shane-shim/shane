# Pages 41-50

Second, there is a clear cluster of low-cost, low-performance models. The Gemma models, which are available for free (at no cost to users), for example, occupy the bottom of the performance scale (21.0-35.5%). While they may be valuable in other contexts, they are currently not competitive for physics conceptual tasks requiring visual interpretation.

Third, several models show an attractive balance between affordability and capability. The "reasoning-enabled" Gemini 2.5 Flash performs in the high 60% range while keeping costs under $1.00. Even more interesting is the performance of the "reasoning-disabled" Gemini 2.5 Flash: at just $0.31, it achieves performance in the low 60% range, making it an interesting competitor for low-cost use. For less demanding applications, GPT 4.1 mini may be a good candidate. Interestingly, it outperformed its larger sibling GPT-4.1 and showed mid-50% range performance at a price of just $0.10. This is 45 times cheaper than Claude Opus 4, which has essentially the same level of performance.

Finally, grouping by provider reveals broader trends. OpenAI's models tend to cluster in the upper-left quadrant, combining solid performance with reasonable costs. Google's lineup is more diverse, ranging from the very capable but expensive Gemini 2.5 Pro to the lower-performing but completely free Gemma models. Anthropic's Claude models fall in the mid or low performance tiers despite being relatively expensive, suggesting a less favorable price-to-performance ratio for this set of tasks.

**Table 4**: Token usage and costs by model. Token usage provides the average number of tokens (input, output, and reasoning where applicable) for one iteration of the benchmark's 102 items. Costs are provided in USD. The last column shows the total cost of running all 102 items once for each model.

[Table reference to original]

Overall, these results suggest that model selection for educational applications should be guided by empirical performance data rather than assumptions about provider reputation or listed per-token pricing. Low-cost models can provide equivalent, comparable, or even superior performance on physics conceptual tasks involving visual representations in some cases.

## 4. Discussion, Limitations, and Future Research

The performance analysis shows that the highest-performing MLLMs are already surpassing post-instruction college student averages on the four tested concept inventories. Some of them are approaching expert-level performance on some inventories. The top models from OpenAI and Google now exceed 75% accuracy on the benchmark, suggesting substantial potential for educational deployment. However, the non-uniform performance across the four basic concept inventories suggests that even the best models still struggle with many items. While the tested models performed quite well collectively on BEMA and TUG-K, scores dropped significantly on QMVI and especially FTGOT. These findings suggest not only the need for future research focused on how specific visual formats and task types affect model performance, but also the need for qualitative research investigating the reasoning behind MLLMs' chosen answers.

The cost analysis shows that performance does not scale linearly with cost. Some of the most capable models remain relatively cost-effective (e.g., o3, o4 mini), while others like Claude Opus 4 had low performance despite high costs. This suggests that institutions cannot rely solely on price tiers (cost per token) or provider reputation as proxies for quality in diverse educational settings. Conversely, certain mid-range or low-cost models offer an attractive balance of performance and affordability. Notably, the "reasoning-disabled" Gemini 2.5 Flash achieved average accuracy above 60% at a fraction of the cost of more expensive models. Our results thus indicate that more affordable models can reach performance levels close to top-performing models. This has important implications for deploying AI in schools or universities operating under financial constraints. However, freely available or open-weight models (e.g., the Gemma 3 series) currently perform well below acceptable thresholds for educational use when physics images are included. While these models may be interesting for other roles, they are not yet suitable for student-facing educational applications or assessment or grading support.

This study provides a broad, comparative view of MLLM performance on conceptual physics tasks involving images, but it also has several limitations.

First, our analysis focused only on multiple-choice items from well-established concept inventories. While this design allows for standardization and comparability, it does not capture how MLLMs perform on more open-ended or unstructured physics tasks, including derivations, written explanations, or laboratory-based data analysis. Future research should expand to include these formats, which are common in authentic classroom and assessment environments.