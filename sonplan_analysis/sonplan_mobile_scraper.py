import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import time
import json

def scrape_mobile_reviews():
    """
    Scrape reviews from Sonplan mobile site
    """
    base_url = "https://m.sonplan.com/product/%EC%8D%AC%ED%94%8C%EB%9E%9C-%ED%83%80%EC%9E%84%EC%8A%AC%EB%A6%BD-%EC%95%84%EC%9D%B4-%ED%81%AC%EB%A6%BC-220g/10/category/23/display/1/"
    
    # Mobile user agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    session = requests.Session()
    reviews_data = []
    
    print("ëª¨ë°”ì¼ í˜ì´ì§€ì—ì„œ ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘...")
    
    # First, get the main page
    response = session.get(base_url, headers=headers)
    print(f"ë©”ì¸ í˜ì´ì§€ ì‘ë‹µ: {response.status_code}")
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Save debug HTML
        with open('mobile_page_debug.html', 'w', encoding='utf-8') as f:
            f.write(soup.prettify())
        print("ë””ë²„ê·¸ìš© HTML ì €ì¥: mobile_page_debug.html")
        
        # Try different patterns for mobile review sections
        review_patterns = [
            # Common mobile review section IDs/classes
            soup.find('div', id='prdReview'),
            soup.find('div', class_='xans-product-review'),
            soup.find('div', class_='board-review'),
            soup.find('section', class_='review'),
            soup.find('div', class_='review-list'),
            soup.find('ul', class_='review-list'),
            soup.find('div', class_='board-list-review')
        ]
        
        review_section = None
        for pattern in review_patterns:
            if pattern:
                review_section = pattern
                print(f"ë¦¬ë·° ì„¹ì…˜ ë°œê²¬: {pattern.name} with class/id: {pattern.get('class', pattern.get('id'))}")
                break
        
        # If review section found, extract reviews
        if review_section:
            # Look for individual review items
            review_items = review_section.find_all(['li', 'div', 'article'], class_=re.compile('review|item|board'))
            
            if not review_items:
                # Try without class filter
                review_items = review_section.find_all(['li', 'div', 'tr'])
            
            print(f"ë°œê²¬ëœ ë¦¬ë·° ì•„ì´í…œ ìˆ˜: {len(review_items)}")
            
            for item in review_items:
                review_data = extract_review_from_element(item)
                if review_data and review_data['review_text']:
                    reviews_data.append(review_data)
        
        # Check for iframe with reviews
        iframes = soup.find_all('iframe')
        for iframe in iframes:
            src = iframe.get('src', '')
            if 'review' in src or 'board' in src:
                print(f"ë¦¬ë·° iframe ë°œê²¬: {src}")
                if not src.startswith('http'):
                    src = 'https://m.sonplan.com' + src
                
                # Get iframe content
                iframe_response = session.get(src, headers=headers)
                if iframe_response.status_code == 200:
                    iframe_soup = BeautifulSoup(iframe_response.content, 'html.parser')
                    iframe_reviews = extract_reviews_from_soup(iframe_soup)
                    reviews_data.extend(iframe_reviews)
        
        # Check for AJAX endpoints in scripts
        scripts = soup.find_all('script')
        for script in scripts:
            content = script.string or ''
            
            # Look for API endpoints
            api_matches = re.findall(r'["\']([^"\']*(?:review|board)[^"\']*)["\']', content)
            for match in api_matches:
                if match.startswith('/') and 'api' in match:
                    print(f"ë°œê²¬ëœ API ì—”ë“œí¬ì¸íŠ¸: {match}")
        
        # Try common mobile board URLs
        if not reviews_data:
            print("\nì¼ë°˜ì ì¸ ëª¨ë°”ì¼ ë¦¬ë·° URL ì‹œë„ ì¤‘...")
            
            mobile_review_urls = [
                f"https://m.sonplan.com/board/product/list.html?board_no=4&product_no=10",
                f"https://m.sonplan.com/board/review/list.html?product_no=10",
                f"https://m.sonplan.com/board/free/list.html?board_no=4"
            ]
            
            for review_url in mobile_review_urls:
                print(f"ì‹œë„: {review_url}")
                try:
                    review_response = session.get(review_url, headers=headers)
                    if review_response.status_code == 200:
                        review_soup = BeautifulSoup(review_response.content, 'html.parser')
                        
                        # Extract reviews
                        page_reviews = extract_reviews_from_soup(review_soup)
                        if page_reviews:
                            reviews_data.extend(page_reviews)
                            print(f"ì¶”ì¶œëœ ë¦¬ë·° ìˆ˜: {len(page_reviews)}")
                            
                            # Try to get more pages
                            for page in range(2, 6):
                                page_url = f"{review_url}&page={page}"
                                try:
                                    page_response = session.get(page_url, headers=headers)
                                    if page_response.status_code == 200:
                                        page_soup = BeautifulSoup(page_response.content, 'html.parser')
                                        more_reviews = extract_reviews_from_soup(page_soup)
                                        if more_reviews:
                                            reviews_data.extend(more_reviews)
                                        else:
                                            break
                                    time.sleep(0.5)
                                except:
                                    break
                            break
                except Exception as e:
                    print(f"ì—ëŸ¬: {e}")
                    continue
    
    return reviews_data

def extract_review_from_element(element):
    """
    Extract review data from an HTML element
    """
    review_text = ""
    rating = 5
    reviewer = "ê³ ê°"
    date = ""
    
    # Extract text content
    # Look for specific content containers
    content_containers = element.find_all(class_=re.compile('content|text|comment|description|subject'))
    if content_containers:
        review_text = ' '.join([c.get_text(strip=True) for c in content_containers])
    else:
        # Get all text but filter out metadata
        all_text = element.get_text(separator=' ', strip=True)
        # Remove common metadata patterns
        review_text = re.sub(r'(ë²ˆí˜¸|ì‘ì„±ì|ë‚ ì§œ|í‰ì |ì¡°íšŒìˆ˜)[\s:]\S+', '', all_text)
        review_text = review_text.strip()
    
    # Extract rating
    rating_elem = element.find(class_=re.compile('star|rating|score'))
    if rating_elem:
        # Count star images
        stars = rating_elem.find_all('img', src=re.compile('star'))
        if stars:
            rating = len([s for s in stars if 'full' in s.get('src', '') or 'on' in s.get('src', '')])
        else:
            # Try to extract from text
            rating_text = rating_elem.get_text()
            rating_match = re.search(r'(\d+)ì |â˜…+|(\d+)/5', rating_text)
            if rating_match:
                if rating_match.group(1):
                    rating = int(rating_match.group(1))
                elif 'â˜…' in rating_match.group(0):
                    rating = len(rating_match.group(0))
    
    # Extract reviewer
    reviewer_elem = element.find(class_=re.compile('writer|name|user|author'))
    if reviewer_elem:
        reviewer = reviewer_elem.get_text(strip=True)
        # Anonymize if needed
        if len(reviewer) > 2:
            reviewer = reviewer[:2] + '*' * (len(reviewer) - 2)
    
    # Extract date
    date_elem = element.find(class_=re.compile('date|time|created'))
    if date_elem:
        date = date_elem.get_text(strip=True)
    
    # Clean up review text
    if review_text:
        # Remove excessive whitespace
        review_text = ' '.join(review_text.split())
        # Remove navigation text
        nav_patterns = ['ì´ì „ê¸€', 'ë‹¤ìŒê¸€', 'ëª©ë¡', 'ê¸€ì“°ê¸°', 'ë‹µë³€', 'ìˆ˜ì •', 'ì‚­ì œ']
        for pattern in nav_patterns:
            review_text = review_text.replace(pattern, '')
        review_text = review_text.strip()
    
    return {
        'review_text': review_text,
        'rating': rating,
        'reviewer': reviewer,
        'date': date
    }

def extract_reviews_from_soup(soup):
    """
    Extract all reviews from a BeautifulSoup object
    """
    reviews = []
    
    # Look for review table
    tables = soup.find_all('table', class_=re.compile('board|list'))
    for table in tables:
        rows = table.find_all('tr')
        for row in rows[1:]:  # Skip header
            cols = row.find_all(['td', 'th'])
            if len(cols) >= 2:
                review_data = {
                    'review_text': '',
                    'rating': 5,
                    'reviewer': 'ê³ ê°',
                    'date': ''
                }
                
                # Usually: number, subject, writer, date, views
                if len(cols) >= 2:
                    # Subject is usually the second column
                    subject_cell = cols[1]
                    link = subject_cell.find('a')
                    if link:
                        review_data['review_text'] = link.get_text(strip=True)
                    else:
                        review_data['review_text'] = subject_cell.get_text(strip=True)
                
                if len(cols) >= 3:
                    review_data['reviewer'] = cols[2].get_text(strip=True)
                
                if len(cols) >= 4:
                    review_data['date'] = cols[3].get_text(strip=True)
                
                # Extract rating if present
                rating_elem = row.find(class_=re.compile('star|rating'))
                if rating_elem:
                    stars = len(rating_elem.find_all(class_=re.compile('on|full')))
                    if stars > 0:
                        review_data['rating'] = stars
                
                if review_data['review_text'] and len(review_data['review_text']) > 5:
                    reviews.append(review_data)
    
    # Look for review lists (ul/ol)
    review_lists = soup.find_all(['ul', 'ol'], class_=re.compile('review|board'))
    for review_list in review_lists:
        items = review_list.find_all('li')
        for item in items:
            review_data = extract_review_from_element(item)
            if review_data['review_text']:
                reviews.append(review_data)
    
    # Look for review divs
    review_divs = soup.find_all('div', class_=re.compile('review-item|board-item|list-item'))
    for div in review_divs:
        review_data = extract_review_from_element(div)
        if review_data['review_text']:
            reviews.append(review_data)
    
    return reviews

def analyze_reviews(reviews_data):
    """
    Analyze collected reviews
    """
    if not reviews_data:
        return None, None, None
    
    # Convert to DataFrame
    df = pd.DataFrame(reviews_data)
    
    # Remove duplicates
    df = df.drop_duplicates(subset=['review_text'])
    
    # Keyword analysis
    all_words = []
    stop_words = ['ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', 'ê°™ì•„ìš”', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì œ', 'ë”', 'ë°', 'ë•Œ', 'ë“±', 'ë°', 'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 'ê¹Œì§€', 'í•´ìš”', 'í•˜ê³ ', 'í–ˆì–´ìš”', 'ì…ë‹ˆë‹¤', 'ì—ìš”', 'ì˜ˆìš”', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ë˜ì–´', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆê³ ', 'ì—†ê³ ', 'ê°™ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ëª¨ë“ ', 'ê°ê°', 'ë²ˆí˜¸', 'ì‘ì„±ì', 'ë‚ ì§œ', 'ì¡°íšŒìˆ˜']
    
    for review in df['review_text']:
        if pd.isna(review):
            continue
        
        # Extract Korean words
        words = re.findall(r'[ê°€-í£]+', str(review))
        words = [word for word in words if 2 <= len(word) <= 6 and word not in stop_words]
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    
    # Get bigrams
    bigrams = []
    for review in df['review_text']:
        if pd.isna(review):
            continue
        words = re.findall(r'[ê°€-í£]+', str(review))
        words = [word for word in words if 2 <= len(word) <= 6]
        for i in range(len(words)-1):
            if words[i] not in stop_words and words[i+1] not in stop_words:
                bigrams.append(f"{words[i]} {words[i+1]}")
    
    bigram_freq = Counter(bigrams)
    
    return df, word_freq, bigram_freq

def visualize_results(df, word_freq, bigram_freq):
    """
    Create visualizations
    """
    if df is None or df.empty:
        return
    
    plt.style.use('seaborn-v0_8-white')
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Top Keywords
    ax1 = axes[0, 0]
    top_words = dict(word_freq.most_common(15))
    if top_words:
        bars = ax1.bar(range(len(top_words)), list(top_words.values()), color='skyblue')
        ax1.set_xticks(range(len(top_words)))
        ax1.set_xticklabels(list(top_words.keys()), rotation=45, ha='right')
        ax1.set_title('ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 15', fontsize=14, fontweight='bold')
        ax1.set_ylabel('ë¹ˆë„ìˆ˜')
        
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom', fontsize=9)
    
    # 2. Bigrams
    ax2 = axes[0, 1]
    top_bigrams = dict(bigram_freq.most_common(10))
    if top_bigrams:
        y_pos = range(len(top_bigrams))
        ax2.barh(y_pos, list(top_bigrams.values()), color='lightcoral')
        ax2.set_yticks(y_pos)
        ax2.set_yticklabels(list(top_bigrams.keys()))
        ax2.set_title('í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© TOP 10', fontsize=14, fontweight='bold')
        ax2.set_xlabel('ë¹ˆë„ìˆ˜')
    
    # 3. Review Length Distribution
    ax3 = axes[1, 0]
    review_lengths = df['review_text'].str.len()
    ax3.hist(review_lengths, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)
    ax3.set_title('ë¦¬ë·° ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')
    ax3.set_xlabel('ê¸€ì ìˆ˜')
    ax3.set_ylabel('ë¦¬ë·° ìˆ˜')
    mean_length = review_lengths.mean()
    ax3.axvline(mean_length, color='red', linestyle='dashed', linewidth=2, 
                label=f'í‰ê· : {mean_length:.0f}ì')
    ax3.legend()
    
    # 4. Summary
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    total_reviews = len(df)
    unique_keywords = len(word_freq)
    avg_rating = df['rating'].mean()
    
    summary_text = f"""
    ğŸ“Š ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ë¶„ì„ ìš”ì•½
    
    ì´ ë¦¬ë·° ìˆ˜: {total_reviews}ê°œ
    í‰ê·  í‰ì : {avg_rating:.2f}/5.0
    í‰ê·  ë¦¬ë·° ê¸¸ì´: {mean_length:.0f}ì
    ì¶”ì¶œëœ ê³ ìœ  í‚¤ì›Œë“œ: {unique_keywords}ê°œ
    
    ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:
    â€¢ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ:
      {', '.join(list(top_words.keys())[:5])}
    
    â€¢ ê³ ê°ë“¤ì´ í•¨ê»˜ ì–¸ê¸‰í•˜ëŠ” í‘œí˜„:
      {', '.join(list(top_bigrams.keys())[:3]) if top_bigrams else 'ì—†ìŒ'}
    """
    
    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, 
             fontsize=12, verticalalignment='top', fontfamily='AppleGothic')
    
    plt.tight_layout()
    plt.savefig('sonplan_mobile_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Word Cloud
    if word_freq:
        plt.figure(figsize=(10, 6))
        wordcloud = WordCloud(
            font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
            background_color='white',
            width=1000,
            height=600,
            max_words=50,
            colormap='viridis'
        ).generate_from_frequencies(dict(word_freq))
        
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.savefig('sonplan_mobile_wordcloud.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    print("ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ - ëª¨ë°”ì¼ í˜ì´ì§€ ë¦¬ë·° ë¶„ì„")
    print("=" * 50)
    
    # Scrape reviews from mobile site
    reviews_data = scrape_mobile_reviews()
    
    if not reviews_data:
        print("\në¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ëª¨ë°”ì¼ í˜ì´ì§€ë„ ë™ì  ë¡œë”©ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì¸ì¦ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nâœ… ì´ {len(reviews_data)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    
    # Analyze reviews
    df, word_freq, bigram_freq = analyze_reviews(reviews_data)
    
    if df is None or df.empty:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Save data
    df.to_csv('sonplan_mobile_reviews.csv', index=False, encoding='utf-8-sig')
    print("\nğŸ’¾ ë¦¬ë·° ë°ì´í„°ë¥¼ 'sonplan_mobile_reviews.csv'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    
    # Display samples
    print("\nğŸ“ ë¦¬ë·° ìƒ˜í”Œ:")
    for i, row in df.head(5).iterrows():
        print(f"\n{i+1}. {row['review_text'][:80]}...")
        print(f"   í‰ì : {'â­' * int(row['rating'])}")
        print(f"   ì‘ì„±ì: {row['reviewer']}")
    
    # Print analysis
    print("\nğŸ“Š ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ:")
    for i, (word, freq) in enumerate(word_freq.most_common(20), 1):
        print(f"{i:2d}. {word}: {freq}íšŒ")
    
    # Visualize
    print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    visualize_results(df, word_freq, bigram_freq)
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()