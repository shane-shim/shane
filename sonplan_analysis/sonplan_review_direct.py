import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import time

def find_review_iframe_url():
    """
    Find the actual review iframe URL from the product page
    """
    url = "https://sonplan.com/product/%EC%8D%AC%ED%94%8C%EB%9E%9C-%ED%83%80%EC%9E%84%EC%8A%AC%EB%A6%BD-%EC%95%84%EC%9D%B4-%ED%81%AC%EB%A6%BC-220g/10/category/23/display/1/"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Find iframe sources
    iframes = soup.find_all('iframe')
    review_iframe_url = None
    
    for iframe in iframes:
        src = iframe.get('src', '')
        if 'board' in src or 'review' in src:
            print(f"Found potential review iframe: {src}")
            if not src.startswith('http'):
                src = 'https://sonplan.com' + src
            review_iframe_url = src
            break
    
    # Also check for board URLs in scripts
    scripts = soup.find_all('script')
    for script in scripts:
        content = script.string or ''
        if 'board' in content:
            # Look for board URLs
            board_matches = re.findall(r'/board[^"\']*', content)
            for match in board_matches:
                if 'list' in match:
                    print(f"Found board URL in script: {match}")
    
    return review_iframe_url

def scrape_reviews_from_board():
    """
    Try common Cafe24 board patterns
    """
    base_url = "https://sonplan.com"
    product_no = "10"
    
    # Common Cafe24 review board patterns
    board_urls = [
        f"{base_url}/board/product/list.html?board_no=4&product_no={product_no}",
        f"{base_url}/board/free/list.html?board_no=4&product_no={product_no}",
        f"{base_url}/board/review/list.html?product_no={product_no}",
        f"{base_url}/board/board.html?board_no=4&product_no={product_no}"
    ]
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Referer': 'https://sonplan.com/'
    }
    
    reviews_data = []
    
    for board_url in board_urls:
        print(f"\nTrying: {board_url}")
        
        try:
            response = requests.get(board_url, headers=headers)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Debug: print page title
                title = soup.find('title')
                if title:
                    print(f"Page title: {title.text}")
                
                # Look for board table
                tables = soup.find_all('table')
                for table in tables:
                    # Check if this is a board table
                    ths = table.find_all('th')
                    if any('ë²ˆí˜¸' in th.text for th in ths):
                        print("Found board table!")
                        
                        # Extract reviews from rows
                        rows = table.find_all('tr')[1:]  # Skip header
                        for row in rows:
                            cols = row.find_all('td')
                            if len(cols) >= 3:
                                # Extract review data
                                subject_cell = cols[1]
                                subject_link = subject_cell.find('a')
                                
                                if subject_link:
                                    review_text = subject_link.get_text(strip=True)
                                else:
                                    review_text = subject_cell.get_text(strip=True)
                                
                                if review_text and len(review_text) > 5:
                                    reviewer = cols[2].get_text(strip=True) if len(cols) > 2 else 'Customer'
                                    date = cols[3].get_text(strip=True) if len(cols) > 3 else ''
                                    
                                    reviews_data.append({
                                        'review_text': review_text,
                                        'reviewer': reviewer,
                                        'date': date,
                                        'rating': 5
                                    })
                        
                        if reviews_data:
                            print(f"Successfully extracted {len(reviews_data)} reviews!")
                            
                            # Try to get more pages
                            for page in range(2, 6):  # Get pages 2-5
                                page_url = f"{board_url}&page={page}"
                                try:
                                    page_response = requests.get(page_url, headers=headers)
                                    page_soup = BeautifulSoup(page_response.content, 'html.parser')
                                    
                                    page_table = page_soup.find('table')
                                    if page_table:
                                        page_rows = page_table.find_all('tr')[1:]
                                        for row in page_rows:
                                            cols = row.find_all('td')
                                            if len(cols) >= 3:
                                                subject_cell = cols[1]
                                                review_text = subject_cell.get_text(strip=True)
                                                
                                                if review_text and len(review_text) > 5:
                                                    reviewer = cols[2].get_text(strip=True) if len(cols) > 2 else 'Customer'
                                                    date = cols[3].get_text(strip=True) if len(cols) > 3 else ''
                                                    
                                                    reviews_data.append({
                                                        'review_text': review_text,
                                                        'reviewer': reviewer,
                                                        'date': date,
                                                        'rating': 5
                                                    })
                                    
                                    time.sleep(0.5)  # Be respectful
                                except:
                                    break
                            
                            return pd.DataFrame(reviews_data)
                
                # If no board table found, look for review divs
                review_divs = soup.find_all('div', class_=re.compile('review|board'))
                for div in review_divs:
                    text = div.get_text(strip=True)
                    if len(text) > 20 and len(text) < 500:
                        reviews_data.append({
                            'review_text': text,
                            'reviewer': 'Customer',
                            'date': '',
                            'rating': 5
                        })
                
        except Exception as e:
            print(f"Error: {e}")
            continue
    
    return pd.DataFrame(reviews_data)

def analyze_keywords(df):
    """
    Simple keyword analysis
    """
    all_words = []
    stop_words = ['ìžˆì–´ìš”', 'ìžˆìŠµë‹ˆë‹¤', 'ê°™ì•„ìš”', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì œ', 'ë”', 'ë°', 'ë•Œ', 'ë“±', 'ë°', 'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 'ê¹Œì§€', 'í•´ìš”', 'í•˜ê³ ', 'í–ˆì–´ìš”', 'ìž…ë‹ˆë‹¤', 'ì—ìš”', 'ì˜ˆìš”']
    
    for review in df['review_text']:
        if pd.isna(review):
            continue
        
        # Extract Korean words
        words = re.findall(r'[ê°€-íž£]+', review)
        words = [word for word in words if 2 <= len(word) <= 6 and word not in stop_words]
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    
    # Get bigrams
    bigrams = []
    for review in df['review_text']:
        if pd.isna(review):
            continue
        words = re.findall(r'[ê°€-íž£]+', review)
        words = [word for word in words if 2 <= len(word) <= 6]
        for i in range(len(words)-1):
            if words[i] not in stop_words and words[i+1] not in stop_words:
                bigrams.append(f"{words[i]} {words[i+1]}")
    
    bigram_freq = Counter(bigrams)
    
    return word_freq, bigram_freq

def visualize_analysis(word_freq, bigram_freq, df):
    """
    Create visualizations
    """
    plt.style.use('seaborn-v0_8-white')
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. Top Keywords
    top_words = dict(word_freq.most_common(15))
    ax1 = axes[0, 0]
    bars = ax1.bar(range(len(top_words)), list(top_words.values()), color='skyblue')
    ax1.set_xticks(range(len(top_words)))
    ax1.set_xticklabels(list(top_words.keys()), rotation=45, ha='right')
    ax1.set_title('ê°€ìž¥ ë§Žì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 15', fontsize=14, fontweight='bold')
    ax1.set_ylabel('ë¹ˆë„ìˆ˜')
    
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom', fontsize=9)
    
    # 2. Bigrams
    top_bigrams = dict(bigram_freq.most_common(10))
    ax2 = axes[0, 1]
    if top_bigrams:
        y_pos = range(len(top_bigrams))
        ax2.barh(y_pos, list(top_bigrams.values()), color='lightcoral')
        ax2.set_yticks(y_pos)
        ax2.set_yticklabels(list(top_bigrams.keys()))
        ax2.set_title('í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© TOP 10', fontsize=14, fontweight='bold')
        ax2.set_xlabel('ë¹ˆë„ìˆ˜')
    
    # 3. Review stats
    ax3 = axes[1, 0]
    review_lengths = df['review_text'].str.len()
    ax3.hist(review_lengths, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)
    ax3.set_title('ë¦¬ë·° ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')
    ax3.set_xlabel('ê¸€ìž ìˆ˜')
    ax3.set_ylabel('ë¦¬ë·° ìˆ˜')
    ax3.axvline(review_lengths.mean(), color='red', linestyle='dashed', linewidth=2, 
                label=f'í‰ê· : {review_lengths.mean():.0f}ìž')
    ax3.legend()
    
    # 4. Summary
    ax4 = axes[1, 1]
    ax4.axis('off')
    summary_text = f"""
    ðŸ“Š ë¶„ì„ ìš”ì•½
    
    ì´ ë¦¬ë·° ìˆ˜: {len(df)}ê°œ
    í‰ê·  ë¦¬ë·° ê¸¸ì´: {review_lengths.mean():.0f}ìž
    ì¶”ì¶œëœ ê³ ìœ  í‚¤ì›Œë“œ: {len(word_freq)}ê°œ
    
    ðŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:
    â€¢ ê°€ìž¥ ë§Žì´ ì–¸ê¸‰ëœ 3ê°œ í‚¤ì›Œë“œ:
      {', '.join(list(top_words.keys())[:3])}
    
    â€¢ ê³ ê°ë“¤ì´ ìžì£¼ í•¨ê»˜ ì–¸ê¸‰í•˜ëŠ” í‘œí˜„:
      {', '.join(list(top_bigrams.keys())[:3])}
    """
    
    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, 
             fontsize=12, verticalalignment='top', fontfamily='AppleGothic')
    
    plt.tight_layout()
    plt.savefig('sonplan_review_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Word Cloud
    if word_freq:
        plt.figure(figsize=(12, 8))
        wordcloud = WordCloud(
            font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
            background_color='white',
            width=1200,
            height=800,
            max_words=100,
            relative_scaling=0.5,
            min_font_size=10,
            colormap='viridis'
        ).generate_from_frequencies(dict(word_freq))
        
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('ì¬í”Œëžœ íƒ€ìž„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ', 
                 fontsize=18, fontweight='bold', pad=20)
        plt.savefig('sonplan_wordcloud.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    print("ì¬í”Œëžœ íƒ€ìž„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ë¶„ì„")
    print("=" * 50)
    
    # First try to find iframe URL
    print("\n1. ë¦¬ë·° iframe URL ì°¾ê¸°...")
    iframe_url = find_review_iframe_url()
    
    # Try to scrape reviews
    print("\n2. ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘ ì‹œë„...")
    df_reviews = scrape_reviews_from_board()
    
    if df_reviews.empty:
        print("\në¦¬ë·°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("\nê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:")
        print("1. ë¸Œë¼ìš°ì €ì—ì„œ ê°œë°œìž ë„êµ¬(F12)ë¥¼ ì—´ê³  Network íƒ­ í™•ì¸")
        print("2. ë¦¬ë·° íƒ­ì„ í´ë¦­í•  ë•Œ ë°œìƒí•˜ëŠ” ìš”ì²­ í™•ì¸")
        print("3. 'board' ë˜ëŠ” 'review'ê°€ í¬í•¨ëœ ìš”ì²­ ì°¾ê¸°")
        print("4. í•´ë‹¹ URLê³¼ í•„ìš”í•œ íŒŒë¼ë¯¸í„° í™•ì¸")
        return
    
    print(f"\nâœ… ì´ {len(df_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    
    # Save raw data
    df_reviews.to_csv('sonplan_reviews_raw.csv', index=False, encoding='utf-8-sig')
    print("ì›ë³¸ ë°ì´í„°ë¥¼ 'sonplan_reviews_raw.csv'ì— ì €ìž¥í–ˆìŠµë‹ˆë‹¤.")
    
    # Display sample reviews
    print("\nðŸ“ ë¦¬ë·° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
    for i, row in df_reviews.head().iterrows():
        print(f"\n{i+1}. {row['review_text'][:50]}...")
        print(f"   ìž‘ì„±ìž: {row['reviewer']}, ë‚ ì§œ: {row['date']}")
    
    # Analyze keywords
    print("\nðŸ” í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    word_freq, bigram_freq = analyze_keywords(df_reviews)
    
    # Print top keywords
    print("\nðŸ“Š ê°€ìž¥ ë§Žì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 20:")
    for i, (word, freq) in enumerate(word_freq.most_common(20), 1):
        print(f"{i:2d}. {word}: {freq}íšŒ")
    
    print("\nðŸ”— í•¨ê»˜ ìžì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© TOP 10:")
    for i, (bigram, freq) in enumerate(bigram_freq.most_common(10), 1):
        print(f"{i:2d}. {bigram}: {freq}íšŒ")
    
    # Save analysis results
    keywords_df = pd.DataFrame([
        {'í‚¤ì›Œë“œ': word, 'ë¹ˆë„': freq, 'ìˆœìœ„': i+1}
        for i, (word, freq) in enumerate(word_freq.most_common(100))
    ])
    keywords_df.to_csv('sonplan_keywords_analysis.csv', index=False, encoding='utf-8-sig')
    
    bigrams_df = pd.DataFrame([
        {'ë‹¨ì–´ì¡°í•©': bigram, 'ë¹ˆë„': freq, 'ìˆœìœ„': i+1}
        for i, (bigram, freq) in enumerate(bigram_freq.most_common(50))
    ])
    bigrams_df.to_csv('sonplan_bigrams_analysis.csv', index=False, encoding='utf-8-sig')
    
    print("\nðŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ìž¥:")
    print("- í‚¤ì›Œë“œ ë¶„ì„: sonplan_keywords_analysis.csv")
    print("- ë‹¨ì–´ ì¡°í•© ë¶„ì„: sonplan_bigrams_analysis.csv")
    
    # Visualize
    print("\nðŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    visualize_analysis(word_freq, bigram_freq, df_reviews)
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()