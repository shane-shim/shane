import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import json

def scrape_with_selenium():
    """
    Use Selenium to scrape dynamically loaded reviews
    """
    # Setup Chrome options
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run in background
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    reviews_data = []
    
    try:
        print("Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
        driver = webdriver.Chrome(options=chrome_options)
        
        # Load the page
        url = "https://sonplan.com/product/%EC%8D%AC%ED%94%8C%EB%9E%9C-%ED%83%80%EC%9E%84%EC%8A%AC%EB%A6%BD-%EC%95%84%EC%9D%B4-%ED%81%AC%EB%A6%BC-220g/10/category/23/display/1/"
        driver.get(url)
        
        # Wait for the page to load
        print("í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(5)
        
        # Scroll to review section to trigger loading
        print("ë¦¬ë·° ì„¹ì…˜ìœ¼ë¡œ ìŠ¤í¬ë¡¤...")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
        time.sleep(3)
        
        # Try to find review tab and click it
        try:
            review_tab = driver.find_element(By.CSS_SELECTOR, 'a[href*="#prdReview"], a[href*="#review"], .tab-review, .review-tab')
            driver.execute_script("arguments[0].click();", review_tab)
            time.sleep(2)
            print("ë¦¬ë·° íƒ­ í´ë¦­ ì™„ë£Œ")
        except:
            print("ë¦¬ë·° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê³„ì† ì§„í–‰...")
        
        # Find all review text elements
        print("\n'.widget_item_review_text' í´ë˜ìŠ¤ ìš”ì†Œ ì°¾ëŠ” ì¤‘...")
        review_texts = driver.find_elements(By.CLASS_NAME, 'widget_item_review_text')
        
        if not review_texts:
            # Try other possible selectors
            print("ë‹¤ë¥¸ ì…€ë ‰í„° ì‹œë„ ì¤‘...")
            selectors = [
                '.widget_item.review .text',
                '.widget_item_review_text',
                '.review-text',
                '.review-content',
                '[class*="review"][class*="text"]',
                '.widget_item_review_small .text'
            ]
            
            for selector in selectors:
                review_texts = driver.find_elements(By.CSS_SELECTOR, selector)
                if review_texts:
                    print(f"Found {len(review_texts)} reviews with selector: {selector}")
                    break
        
        print(f"ë°œê²¬ëœ ë¦¬ë·° í…ìŠ¤íŠ¸: {len(review_texts)}ê°œ")
        
        # Extract review data
        for i, review_element in enumerate(review_texts):
            try:
                review_text = review_element.text.strip()
                
                if review_text and len(review_text) > 10:
                    # Try to find associated rating
                    rating = 5  # Default
                    try:
                        # Look for rating in parent element
                        parent = review_element.find_element(By.XPATH, '..')
                        rating_elements = parent.find_elements(By.CSS_SELECTOR, '.star-on, .star-full, [class*="star"][class*="fill"]')
                        if rating_elements:
                            rating = len(rating_elements)
                    except:
                        pass
                    
                    # Try to find reviewer name
                    reviewer = "ê³ ê°"
                    try:
                        parent = review_element.find_element(By.XPATH, '../..')
                        reviewer_elem = parent.find_element(By.CSS_SELECTOR, '.reviewer, .writer, .name, .user')
                        reviewer = reviewer_elem.text.strip()
                    except:
                        pass
                    
                    # Try to find date
                    date = ""
                    try:
                        parent = review_element.find_element(By.XPATH, '../..')
                        date_elem = parent.find_element(By.CSS_SELECTOR, '.date, .time, .created')
                        date = date_elem.text.strip()
                    except:
                        pass
                    
                    reviews_data.append({
                        'review_text': review_text,
                        'rating': rating,
                        'reviewer': reviewer,
                        'date': date
                    })
                    
                    print(f"ë¦¬ë·° {i+1}: {review_text[:50]}...")
                    
            except Exception as e:
                print(f"Error extracting review {i}: {e}")
                continue
        
        # Check if reviews are in iframe
        if not reviews_data:
            print("\niframe ë‚´ë¶€ í™•ì¸ ì¤‘...")
            iframes = driver.find_elements(By.TAG_NAME, 'iframe')
            
            for iframe_idx, iframe in enumerate(iframes):
                try:
                    driver.switch_to.frame(iframe)
                    print(f"iframe {iframe_idx+1} ì§„ì…")
                    
                    # Look for review texts in iframe
                    iframe_reviews = driver.find_elements(By.CLASS_NAME, 'widget_item_review_text')
                    
                    if not iframe_reviews:
                        iframe_reviews = driver.find_elements(By.CSS_SELECTOR, '.review-text, .review-content, [class*="review"][class*="text"]')
                    
                    print(f"iframe ë‚´ë¶€ì—ì„œ {len(iframe_reviews)}ê°œ ë¦¬ë·° ë°œê²¬")
                    
                    for idx, review_elem in enumerate(iframe_reviews):
                        try:
                            review_text = review_elem.text.strip()
                            if not review_text:
                                # Try getting text with JavaScript
                                review_text = driver.execute_script("return arguments[0].innerText || arguments[0].textContent", review_elem)
                            
                            if review_text and len(review_text) > 10:
                                reviews_data.append({
                                    'review_text': review_text.strip(),
                                    'rating': 5,
                                    'reviewer': 'ê³ ê°',
                                    'date': ''
                                })
                                print(f"  ë¦¬ë·° {idx+1}: {review_text[:50]}...")
                        except Exception as e:
                            print(f"  ë¦¬ë·° ì¶”ì¶œ ì˜¤ë¥˜ {idx}: {e}")
                    
                    driver.switch_to.default_content()
                    
                except Exception as e:
                    print(f"iframe error: {e}")
                    driver.switch_to.default_content()
                    continue
        
        # Save page source for debugging
        with open('selenium_page_source.html', 'w', encoding='utf-8') as f:
            f.write(driver.page_source)
        print("\në””ë²„ê·¸ìš© í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥: selenium_page_source.html")
        
        driver.quit()
        
    except Exception as e:
        print(f"Selenium error: {e}")
        return []
    
    return reviews_data

def scrape_with_requests():
    """
    Try to scrape with requests first (faster if it works)
    """
    url = "https://sonplan.com/product/%EC%8D%AC%ED%94%8C%EB%9E%9C-%ED%83%80%EC%9E%84%EC%8A%AC%EB%A6%BD-%EC%95%84%EC%9D%B4-%ED%81%AC%EB%A6%BC-220g/10/category/23/display/1/"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
    }
    
    print("Requestsë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹œë„...")
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find review text elements
        review_elements = soup.find_all(class_='widget_item_review_text')
        print(f"Found {len(review_elements)} elements with class 'widget_item_review_text'")
        
        if not review_elements:
            # Try to find any element containing the class
            all_elements = soup.find_all(class_=re.compile('widget.*review.*text'))
            print(f"Found {len(all_elements)} elements with similar class names")
            
            # Also check in scripts for dynamic content
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'widget_item_review_text' in script.string:
                    print("Found 'widget_item_review_text' in script - content is dynamically loaded")
                    return None
        
        reviews_data = []
        for elem in review_elements:
            review_text = elem.get_text(strip=True)
            if review_text and len(review_text) > 10:
                reviews_data.append({
                    'review_text': review_text,
                    'rating': 5,
                    'reviewer': 'ê³ ê°',
                    'date': ''
                })
        
        return reviews_data
    
    return None

def analyze_reviews(reviews_data):
    """
    Analyze collected reviews
    """
    if not reviews_data:
        return None, None, None
    
    # Convert to DataFrame
    df = pd.DataFrame(reviews_data)
    
    # Remove duplicates
    df = df.drop_duplicates(subset=['review_text'])
    
    print(f"\nì¤‘ë³µ ì œê±° í›„ ë¦¬ë·° ìˆ˜: {len(df)}")
    
    # Keyword analysis
    all_words = []
    stop_words = ['ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', 'ê°™ì•„ìš”', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì œ', 'ë”', 'ë°', 'ë•Œ', 'ë“±', 'ë°', 'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 'ê¹Œì§€', 'í•´ìš”', 'í•˜ê³ ', 'í–ˆì–´ìš”', 'ì…ë‹ˆë‹¤', 'ì—ìš”', 'ì˜ˆìš”']
    
    for review in df['review_text']:
        # Extract Korean words
        words = re.findall(r'[ê°€-í£]+', str(review))
        words = [word for word in words if 2 <= len(word) <= 6 and word not in stop_words]
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    
    # Get bigrams
    bigrams = []
    for review in df['review_text']:
        words = re.findall(r'[ê°€-í£]+', str(review))
        words = [word for word in words if 2 <= len(word) <= 6]
        for i in range(len(words)-1):
            if words[i] not in stop_words and words[i+1] not in stop_words:
                bigrams.append(f"{words[i]} {words[i+1]}")
    
    bigram_freq = Counter(bigrams)
    
    return df, word_freq, bigram_freq

def visualize_results(df, word_freq, bigram_freq):
    """
    Create visualizations
    """
    plt.style.use('seaborn-v0_8-white')
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Top Keywords
    ax1 = axes[0, 0]
    top_words = dict(word_freq.most_common(15))
    if top_words:
        bars = ax1.bar(range(len(top_words)), list(top_words.values()), color='skyblue')
        ax1.set_xticks(range(len(top_words)))
        ax1.set_xticklabels(list(top_words.keys()), rotation=45, ha='right')
        ax1.set_title('ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 15', fontsize=14, fontweight='bold')
        ax1.set_ylabel('ë¹ˆë„ìˆ˜')
        
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height)}', ha='center', va='bottom')
    
    # 2. Bigrams
    ax2 = axes[0, 1]
    top_bigrams = dict(bigram_freq.most_common(10))
    if top_bigrams:
        y_pos = range(len(top_bigrams))
        ax2.barh(y_pos, list(top_bigrams.values()), color='lightcoral')
        ax2.set_yticks(y_pos)
        ax2.set_yticklabels(list(top_bigrams.keys()))
        ax2.set_title('í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© TOP 10', fontsize=14, fontweight='bold')
        ax2.set_xlabel('ë¹ˆë„ìˆ˜')
    
    # 3. Review Length Distribution
    ax3 = axes[1, 0]
    review_lengths = df['review_text'].str.len()
    ax3.hist(review_lengths, bins=20, color='lightgreen', edgecolor='black')
    ax3.set_title('ë¦¬ë·° ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')
    ax3.set_xlabel('ê¸€ì ìˆ˜')
    ax3.set_ylabel('ë¦¬ë·° ìˆ˜')
    ax3.axvline(review_lengths.mean(), color='red', linestyle='dashed', linewidth=2,
                label=f'í‰ê· : {review_lengths.mean():.0f}ì')
    ax3.legend()
    
    # 4. Summary
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    summary_text = f"""
    ğŸ“Š ë¦¬ë·° ë¶„ì„ ìš”ì•½
    
    ì´ ë¦¬ë·° ìˆ˜: {len(df)}ê°œ
    í‰ê·  ë¦¬ë·° ê¸¸ì´: {review_lengths.mean():.0f}ì
    ì¶”ì¶œëœ ê³ ìœ  í‚¤ì›Œë“œ: {len(word_freq)}ê°œ
    
    ğŸ” ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ:
    {', '.join(list(top_words.keys())[:5])}
    
    ğŸ”— ìì£¼ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” í‘œí˜„:
    {', '.join(list(top_bigrams.keys())[:3]) if top_bigrams else 'ì—†ìŒ'}
    """
    
    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes,
             fontsize=12, verticalalignment='top', fontfamily='AppleGothic')
    
    plt.tight_layout()
    plt.savefig('sonplan_final_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Word Cloud
    if word_freq:
        plt.figure(figsize=(10, 6))
        wordcloud = WordCloud(
            font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
            background_color='white',
            width=1000,
            height=600,
            max_words=50
        ).generate_from_frequencies(dict(word_freq))
        
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('ë¦¬ë·° í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ', fontsize=16, fontweight='bold', pad=20)
        plt.savefig('sonplan_final_wordcloud.png', dpi=300, bbox_inches='tight')
        plt.show()

def main():
    print("ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ë¶„ì„")
    print("=" * 50)
    print("'.widget_item_review_text' í´ë˜ìŠ¤ë¡œ ë¦¬ë·° ìˆ˜ì§‘")
    
    # First try with requests
    reviews_data = scrape_with_requests()
    
    # If requests doesn't work, use Selenium
    if not reviews_data:
        print("\nì •ì  í¬ë¡¤ë§ ì‹¤íŒ¨, Selenium ì‚¬ìš©...")
        reviews_data = scrape_with_selenium()
    
    if not reviews_data:
        print("\në¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ê°€ëŠ¥í•œ ì›ì¸:")
        print("1. í´ë˜ìŠ¤ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ")
        print("2. JavaScript ë Œë”ë§ íƒ€ì´ë° ë¬¸ì œ")
        print("3. ì¸ì¦ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ")
        return
    
    print(f"\nâœ… ì´ {len(reviews_data)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    
    # Analyze
    df, word_freq, bigram_freq = analyze_reviews(reviews_data)
    
    if df is None:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Save data
    df.to_csv('sonplan_final_reviews.csv', index=False, encoding='utf-8-sig')
    print("\nğŸ’¾ 'sonplan_final_reviews.csv'ì— ì €ì¥ ì™„ë£Œ")
    
    # Display samples
    print("\nğŸ“ ë¦¬ë·° ìƒ˜í”Œ:")
    for i, row in df.head(5).iterrows():
        print(f"\n{i+1}. {row['review_text'][:100]}...")
        print(f"   í‰ì : {'â­' * int(row['rating'])}")
    
    # Analysis results
    print("\nğŸ“Š ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ:")
    for i, (word, freq) in enumerate(word_freq.most_common(20), 1):
        print(f"{i:2d}. {word}: {freq}íšŒ")
    
    print("\nğŸ”— ë‹¨ì–´ ì¡°í•© TOP 10:")
    for i, (bigram, freq) in enumerate(bigram_freq.most_common(10), 1):
        print(f"{i:2d}. {bigram}: {freq}íšŒ")
    
    # Visualize
    print("\nğŸ“ˆ ì‹œê°í™” ìƒì„± ì¤‘...")
    visualize_results(df, word_freq, bigram_freq)
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()