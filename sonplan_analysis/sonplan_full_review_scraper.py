import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import json
from datetime import datetime

def scrape_all_reviews_selenium():
    """
    Scrape all reviews using Selenium with pagination
    """
    # Setup Chrome options
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # Run in background
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    all_reviews = []
    page_count = 0
    
    try:
        print("Seleniumìœ¼ë¡œ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
        driver = webdriver.Chrome(options=chrome_options)
        
        # Load the page
        url = "https://sonplan.com/product/%EC%8D%AC%ED%94%8C%EB%9E%9C-%ED%83%80%EC%9E%84%EC%8A%AC%EB%A6%BD-%EC%95%84%EC%9D%B4-%ED%81%AC%EB%A6%BC-220g/10/category/23/display/1/"
        driver.get(url)
        
        # Wait for the page to load
        print("í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(5)
        
        # Scroll to review section
        print("ë¦¬ë·° ì„¹ì…˜ìœ¼ë¡œ ìŠ¤í¬ë¡¤...")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
        time.sleep(3)
        
        # Click review tab if exists
        try:
            review_tab = driver.find_element(By.CSS_SELECTOR, 'a[href*="#prdReview"], a[href*="#review"], .tab-review, .review-tab, .bs_btn_prddetail_review')
            driver.execute_script("arguments[0].click();", review_tab)
            time.sleep(3)
            print("ë¦¬ë·° íƒ­ í´ë¦­ ì™„ë£Œ")
        except:
            print("ë¦¬ë·° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê³„ì† ì§„í–‰...")
        
        # Find the iframe containing reviews
        print("\në¦¬ë·°ê°€ í¬í•¨ëœ iframe ì°¾ëŠ” ì¤‘...")
        iframes = driver.find_elements(By.TAG_NAME, 'iframe')
        review_iframe = None
        
        for idx, iframe in enumerate(iframes):
            try:
                driver.switch_to.frame(iframe)
                
                # Check if this iframe contains reviews
                review_elements = driver.find_elements(By.CSS_SELECTOR, '.widget_item_review_text, .review-text, .review-content, [class*="review"][class*="text"]')
                
                if review_elements:
                    print(f"iframe {idx+1}ì—ì„œ ë¦¬ë·° ë°œê²¬!")
                    review_iframe = idx
                    driver.switch_to.default_content()
                    break
                
                driver.switch_to.default_content()
            except:
                driver.switch_to.default_content()
                continue
        
        if review_iframe is None:
            print("ë¦¬ë·° iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            driver.quit()
            return []
        
        # Switch to review iframe
        driver.switch_to.frame(driver.find_elements(By.TAG_NAME, 'iframe')[review_iframe])
        time.sleep(2)
        
        # Start pagination
        while True:
            page_count += 1
            print(f"\ní˜ì´ì§€ {page_count} í¬ë¡¤ë§ ì¤‘...")
            
            # Find all reviews on current page
            review_elements = driver.find_elements(By.CSS_SELECTOR, '.widget_item_review_text, .review-text, .review-content, .review_content, .text')
            
            if not review_elements:
                print("ë¦¬ë·° ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # Extract reviews from current page
            page_reviews = []
            for idx, review_elem in enumerate(review_elements):
                try:
                    # Get review text
                    review_text = review_elem.text.strip()
                    if not review_text:
                        review_text = driver.execute_script("return arguments[0].innerText || arguments[0].textContent", review_elem)
                    
                    if review_text and len(review_text) > 10:
                        # Try to get parent element for additional info
                        try:
                            parent = review_elem.find_element(By.XPATH, '../..')
                            
                            # Extract rating
                            rating = 5
                            try:
                                star_elements = parent.find_elements(By.CSS_SELECTOR, '.star-on, .star-full, [class*="star"][class*="fill"], .on')
                                if star_elements:
                                    rating = len(star_elements)
                                else:
                                    # Check for rating in style
                                    star_container = parent.find_element(By.CSS_SELECTOR, '[class*="star"], [class*="rating"]')
                                    style = star_container.get_attribute('style')
                                    if style and 'width' in style:
                                        width_match = re.search(r'width:\s*(\d+)%', style)
                                        if width_match:
                                            rating = int(int(width_match.group(1)) / 20)
                            except:
                                pass
                            
                            # Extract reviewer
                            reviewer = "ê³ ê°"
                            try:
                                reviewer_elem = parent.find_element(By.CSS_SELECTOR, '.reviewer, .writer, .name, .user, .nickname')
                                reviewer = reviewer_elem.text.strip()
                            except:
                                pass
                            
                            # Extract date
                            date = ""
                            try:
                                date_elem = parent.find_element(By.CSS_SELECTOR, '.date, .time, .created, .write_date')
                                date = date_elem.text.strip()
                            except:
                                pass
                            
                        except:
                            parent = None
                            rating = 5
                            reviewer = "ê³ ê°"
                            date = ""
                        
                        review_data = {
                            'review_text': review_text.strip(),
                            'rating': rating,
                            'reviewer': reviewer,
                            'date': date,
                            'page': page_count
                        }
                        
                        page_reviews.append(review_data)
                        
                except Exception as e:
                    continue
            
            print(f"  í˜„ì¬ í˜ì´ì§€ì—ì„œ {len(page_reviews)}ê°œ ë¦¬ë·° ì¶”ì¶œ")
            all_reviews.extend(page_reviews)
            print(f"  ëˆ„ì  ë¦¬ë·° ìˆ˜: {len(all_reviews)}ê°œ")
            
            # Find and click next page button
            try:
                # Common pagination selectors
                next_selectors = [
                    'a.next',
                    'button.next',
                    '.pagination .next',
                    'a[class*="next"]',
                    'button[class*="next"]',
                    '.paging a.next',
                    'a[title="ë‹¤ìŒ"]',
                    'a:contains("ë‹¤ìŒ")',
                    '.widget_item_pagination a.next',
                    '.pagination li:last-child a',
                    'a[onclick*="page"]'
                ]
                
                next_button = None
                for selector in next_selectors:
                    try:
                        if ':contains' in selector:
                            # Use XPath for text content
                            next_button = driver.find_element(By.XPATH, '//a[contains(text(), "ë‹¤ìŒ")]')
                        else:
                            next_button = driver.find_element(By.CSS_SELECTOR, selector)
                        
                        if next_button and next_button.is_displayed() and next_button.is_enabled():
                            break
                        else:
                            next_button = None
                    except:
                        continue
                
                if not next_button:
                    # Try to find page numbers
                    page_links = driver.find_elements(By.CSS_SELECTOR, '.pagination a, .paging a, .widget_item_pagination a')
                    current_page_num = page_count
                    next_page_found = False
                    
                    for link in page_links:
                        try:
                            link_text = link.text.strip()
                            if link_text.isdigit() and int(link_text) == current_page_num + 1:
                                next_button = link
                                next_page_found = True
                                break
                        except:
                            continue
                    
                    if not next_page_found:
                        print("ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        break
                
                # Check if next button is disabled
                if next_button:
                    classes = next_button.get_attribute('class') or ''
                    if 'disabled' in classes or 'inactive' in classes:
                        print("ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                        break
                    
                    # Click next page
                    driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", next_button)
                    time.sleep(3)  # Wait for page to load
                    
                else:
                    print("ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                    
            except TimeoutException:
                print("í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼")
                break
            except Exception as e:
                print(f"í˜ì´ì§€ë„¤ì´ì…˜ ì˜¤ë¥˜: {e}")
                break
            
            # Stop if we have enough reviews or no new reviews
            if len(all_reviews) >= 20000:
                print(f"\nëª©í‘œ ë¦¬ë·° ìˆ˜(20,000ê°œ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                break
            
            # Safety check: stop if we're stuck on the same page
            if len(page_reviews) == 0:
                print("í˜„ì¬ í˜ì´ì§€ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
        
        driver.quit()
        
    except Exception as e:
        print(f"Selenium error: {e}")
        if 'driver' in locals():
            driver.quit()
        return all_reviews
    
    return all_reviews

def analyze_large_dataset(reviews_data):
    """
    Analyze large review dataset
    """
    if not reviews_data:
        return None, None, None, None
    
    # Convert to DataFrame
    df = pd.DataFrame(reviews_data)
    
    # Remove exact duplicates
    original_count = len(df)
    df = df.drop_duplicates(subset=['review_text'])
    print(f"\nì¤‘ë³µ ì œê±°: {original_count}ê°œ â†’ {len(df)}ê°œ")
    
    # Basic statistics
    print(f"\n=== ê¸°ë³¸ í†µê³„ ===")
    print(f"ì´ ë¦¬ë·° ìˆ˜: {len(df):,}ê°œ")
    print(f"í‰ê·  í‰ì : {df['rating'].mean():.2f}")
    print(f"í‰ì  ë¶„í¬:")
    rating_dist = df['rating'].value_counts().sort_index()
    for rating, count in rating_dist.items():
        print(f"  {rating}ì : {count:,}ê°œ ({count/len(df)*100:.1f}%)")
    
    # Date analysis if available
    if df['date'].notna().any():
        try:
            df['date_parsed'] = pd.to_datetime(df['date'], errors='coerce')
            valid_dates = df[df['date_parsed'].notna()]
            if len(valid_dates) > 0:
                print(f"\në‚ ì§œ ë²”ìœ„: {valid_dates['date_parsed'].min()} ~ {valid_dates['date_parsed'].max()}")
        except:
            pass
    
    # Keyword analysis
    print("\ní‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    all_words = []
    stop_words = ['ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', 'ê°™ì•„ìš”', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì œ', 'ë”', 'ë°', 'ë•Œ', 'ë“±', 'ë°', 'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 'ê¹Œì§€', 'í•´ìš”', 'í•˜ê³ ', 'í–ˆì–´ìš”', 'ì…ë‹ˆë‹¤', 'ì—ìš”', 'ì˜ˆìš”', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ë˜ì–´', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆê³ ', 'ì—†ê³ ', 'ê°™ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ëª¨ë“ ', 'ê°ê°', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•´', 'ê´€í•´', 'ë˜í•œ', 'ì—­ì‹œ', 'ì•„ì£¼', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ']
    
    for idx, review in enumerate(df['review_text']):
        if idx % 1000 == 0:
            print(f"  {idx:,}/{len(df):,} ë¦¬ë·° ì²˜ë¦¬ ì¤‘...")
        
        # Extract Korean words
        words = re.findall(r'[ê°€-í£]+', str(review))
        words = [word for word in words if 2 <= len(word) <= 6 and word not in stop_words]
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    
    # Get bigrams (limited for performance)
    print("\në‹¨ì–´ ì¡°í•© ë¶„ì„ ì¤‘...")
    bigrams = []
    sample_size = min(5000, len(df))  # Analyze sample for bigrams
    sample_df = df.sample(n=sample_size, random_state=42)
    
    for review in sample_df['review_text']:
        words = re.findall(r'[ê°€-í£]+', str(review))
        words = [word for word in words if 2 <= len(word) <= 6]
        for i in range(len(words)-1):
            if words[i] not in stop_words and words[i+1] not in stop_words:
                bigrams.append(f"{words[i]} {words[i+1]}")
    
    bigram_freq = Counter(bigrams)
    
    # Categorize keywords
    categories = {
        'í”¼ë¶€/íš¨ê³¼': ['í”¼ë¶€', 'ì£¼ë¦„', 'ëˆˆê°€', 'íƒ„ë ¥', 'ê°œì„ ', 'íš¨ê³¼', 'ì¢‹ì•„', 'ì¢‹ì€', 'ë§Œì¡±', 'ì¶”ì²œ', 'ë³€í™”', 'ëŠë‚Œ'],
        'í…ìŠ¤ì²˜/ì‚¬ìš©ê°': ['ì´‰ì´‰', 'ë¶€ë“œëŸ¬', 'í¡ìˆ˜', 'ë°œë¦¼', 'ëˆì ', 'ê°€ë²¼', 'ì«€ì«€', 'ë¬´ê±°', 'ì‚°ëœ»', 'í…ìŠ¤ì²˜', 'ì œí˜•'],
        'ì„±ë¶„/ì•ˆì „ì„±': ['ì„±ë¶„', 'í–¥', 'ëƒ„ìƒˆ', 'ìê·¹', 'ìˆœí•œ', 'ë¯¼ê°', 'ì•ŒëŸ¬ì§€', 'íŠ¸ëŸ¬ë¸”', 'ì•ˆì „', 'ì²œì—°'],
        'ê°€ê²©/ê°€ì¹˜': ['ê°€ê²©', 'ê°€ì„±ë¹„', 'ë¹„ì‹¸', 'ì €ë ´', 'êµ¬ë§¤', 'ì¬êµ¬ë§¤', 'ì„¸ì¼', 'í• ì¸', 'ëˆ', 'ê°€ì¹˜']
    }
    
    category_analysis = {}
    for category, keywords in categories.items():
        category_words = {}
        for word, freq in word_freq.items():
            if any(keyword in word for keyword in keywords):
                category_words[word] = freq
        category_analysis[category] = {
            'keywords': category_words,
            'total_mentions': sum(category_words.values())
        }
    
    return df, word_freq, bigram_freq, category_analysis

def visualize_large_dataset(df, word_freq, bigram_freq, category_analysis):
    """
    Create comprehensive visualizations for large dataset
    """
    plt.style.use('seaborn-v0_8-white')
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
    
    # Create a comprehensive figure
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Top Keywords
    ax1 = plt.subplot(3, 3, 1)
    top_words = dict(word_freq.most_common(20))
    bars = ax1.bar(range(len(top_words)), list(top_words.values()), color='skyblue')
    ax1.set_xticks(range(len(top_words)))
    ax1.set_xticklabels(list(top_words.keys()), rotation=45, ha='right')
    ax1.set_title('ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 20', fontsize=14, fontweight='bold')
    ax1.set_ylabel('ë¹ˆë„ìˆ˜')
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height):,}', ha='center', va='bottom', fontsize=8)
    
    # 2. Rating Distribution
    ax2 = plt.subplot(3, 3, 2)
    rating_counts = df['rating'].value_counts().sort_index()
    colors = ['#ff4444', '#ff7744', '#ffaa44', '#44ff44', '#00ff00']
    bars = ax2.bar(rating_counts.index, rating_counts.values, 
                   color=[colors[min(int(r)-1, 4)] for r in rating_counts.index])
    ax2.set_title(f'í‰ì  ë¶„í¬ (ì´ {len(df):,}ê°œ ë¦¬ë·°)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('í‰ì ')
    ax2.set_ylabel('ë¦¬ë·° ìˆ˜')
    ax2.set_xticks(rating_counts.index)
    
    total = sum(rating_counts.values)
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height):,}\n({height/total*100:.1f}%)', 
                ha='center', va='bottom', fontsize=9)
    
    # 3. Category Analysis
    ax3 = plt.subplot(3, 3, 3)
    categories = list(category_analysis.keys())
    mentions = [category_analysis[cat]['total_mentions'] for cat in categories]
    colors_cat = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']
    
    bars = ax3.bar(categories, mentions, color=colors_cat)
    ax3.set_title('ì¹´í…Œê³ ë¦¬ë³„ ì–¸ê¸‰ ë¹ˆë„', fontsize=14, fontweight='bold')
    ax3.set_ylabel('ì´ ì–¸ê¸‰ íšŸìˆ˜')
    ax3.set_xticklabels(categories, rotation=15, ha='right')
    
    for bar in bars:
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height):,}', ha='center', va='bottom')
    
    # 4. Bigrams
    ax4 = plt.subplot(3, 3, 4)
    top_bigrams = dict(bigram_freq.most_common(15))
    if top_bigrams:
        y_pos = range(len(top_bigrams))
        ax4.barh(y_pos, list(top_bigrams.values()), color='lightcoral')
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels(list(top_bigrams.keys()))
        ax4.set_title('í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì¡°í•© TOP 15', fontsize=14, fontweight='bold')
        ax4.set_xlabel('ë¹ˆë„ìˆ˜')
    
    # 5. Review Length Distribution
    ax5 = plt.subplot(3, 3, 5)
    review_lengths = df['review_text'].str.len()
    ax5.hist(review_lengths, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)
    ax5.set_title('ë¦¬ë·° ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')
    ax5.set_xlabel('ê¸€ì ìˆ˜')
    ax5.set_ylabel('ë¦¬ë·° ìˆ˜')
    mean_length = review_lengths.mean()
    median_length = review_lengths.median()
    ax5.axvline(mean_length, color='red', linestyle='dashed', linewidth=2, 
                label=f'í‰ê· : {mean_length:.0f}ì')
    ax5.axvline(median_length, color='blue', linestyle='dashed', linewidth=2, 
                label=f'ì¤‘ì•™ê°’: {median_length:.0f}ì')
    ax5.legend()
    
    # 6-9. Category-specific keywords
    for idx, (category, data) in enumerate(category_analysis.items()):
        ax = plt.subplot(3, 3, 6 + idx)
        top_cat_words = dict(sorted(data['keywords'].items(), 
                                   key=lambda x: x[1], reverse=True)[:10])
        if top_cat_words:
            bars = ax.bar(range(len(top_cat_words)), list(top_cat_words.values()), 
                         color=colors_cat[idx])
            ax.set_xticks(range(len(top_cat_words)))
            ax.set_xticklabels(list(top_cat_words.keys()), rotation=45, ha='right')
            ax.set_title(f'{category} ê´€ë ¨ í‚¤ì›Œë“œ', fontsize=12, fontweight='bold')
            ax.set_ylabel('ë¹ˆë„ìˆ˜')
    
    plt.tight_layout()
    plt.savefig('sonplan_full_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Create Word Cloud
    print("\nì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘...")
    plt.figure(figsize=(14, 8))
    
    # Limit words for performance
    top_500_words = dict(word_freq.most_common(500))
    
    wordcloud = WordCloud(
        font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
        background_color='white',
        width=1400,
        height=800,
        max_words=200,
        relative_scaling=0.5,
        min_font_size=10,
        colormap='viridis'
    ).generate_from_frequencies(top_500_words)
    
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ ({len(df):,}ê°œ ë¦¬ë·° ê¸°ë°˜)', 
             fontsize=20, fontweight='bold', pad=20)
    plt.savefig('sonplan_full_wordcloud.png', dpi=300, bbox_inches='tight')
    plt.show()

def save_analysis_report(df, word_freq, bigram_freq, category_analysis):
    """
    Save comprehensive analysis report
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save raw reviews
    print(f"\në¦¬ë·° ë°ì´í„° ì €ì¥ ì¤‘...")
    df.to_csv(f'sonplan_all_reviews_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Save keyword analysis
    keywords_df = pd.DataFrame([
        {'í‚¤ì›Œë“œ': word, 'ë¹ˆë„': freq, 'ìˆœìœ„': i+1}
        for i, (word, freq) in enumerate(word_freq.most_common(1000))
    ])
    keywords_df.to_csv(f'sonplan_keywords_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Save bigram analysis
    bigrams_df = pd.DataFrame([
        {'ë‹¨ì–´ì¡°í•©': bigram, 'ë¹ˆë„': freq, 'ìˆœìœ„': i+1}
        for i, (bigram, freq) in enumerate(bigram_freq.most_common(500))
    ])
    bigrams_df.to_csv(f'sonplan_bigrams_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Save category analysis
    category_data = []
    for category, data in category_analysis.items():
        for word, freq in data['keywords'].items():
            category_data.append({
                'ì¹´í…Œê³ ë¦¬': category,
                'í‚¤ì›Œë“œ': word,
                'ë¹ˆë„': freq
            })
    category_df = pd.DataFrame(category_data)
    category_df = category_df.sort_values(['ì¹´í…Œê³ ë¦¬', 'ë¹ˆë„'], ascending=[True, False])
    category_df.to_csv(f'sonplan_categories_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Create summary report
    with open(f'sonplan_analysis_report_{timestamp}.txt', 'w', encoding='utf-8') as f:
        f.write("ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ ë¦¬ë·° ë¶„ì„ ë³´ê³ ì„œ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
        f.write(f"ì´ ë¦¬ë·° ìˆ˜: {len(df):,}ê°œ\n")
        f.write(f"í‰ê·  í‰ì : {df['rating'].mean():.2f}/5.0\n")
        f.write(f"í‰ê·  ë¦¬ë·° ê¸¸ì´: {df['review_text'].str.len().mean():.0f}ì\n\n")
        
        f.write("í‰ì  ë¶„í¬:\n")
        rating_dist = df['rating'].value_counts().sort_index()
        for rating, count in rating_dist.items():
            f.write(f"  {rating}ì : {count:,}ê°œ ({count/len(df)*100:.1f}%)\n")
        
        f.write("\nìƒìœ„ 50ê°œ í‚¤ì›Œë“œ:\n")
        for i, (word, freq) in enumerate(word_freq.most_common(50), 1):
            f.write(f"  {i:2d}. {word}: {freq:,}íšŒ\n")
        
        f.write("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„:\n")
        for category, data in category_analysis.items():
            f.write(f"\n[{category}] - ì´ {data['total_mentions']:,}íšŒ ì–¸ê¸‰\n")
            top_10 = dict(sorted(data['keywords'].items(), 
                                key=lambda x: x[1], reverse=True)[:10])
            for word, freq in top_10.items():
                f.write(f"  - {word}: {freq:,}íšŒ\n")
    
    print(f"\në¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"  - ì „ì²´ ë¦¬ë·°: sonplan_all_reviews_{timestamp}.csv")
    print(f"  - í‚¤ì›Œë“œ ë¶„ì„: sonplan_keywords_{timestamp}.csv")
    print(f"  - ë‹¨ì–´ ì¡°í•©: sonplan_bigrams_{timestamp}.csv")
    print(f"  - ì¹´í…Œê³ ë¦¬ ë¶„ì„: sonplan_categories_{timestamp}.csv")
    print(f"  - ì¢…í•© ë³´ê³ ì„œ: sonplan_analysis_report_{timestamp}.txt")

def main():
    print("ì¬í”Œëœ íƒ€ì„ìŠ¬ë¦½ ì•„ì´í¬ë¦¼ - ì „ì²´ ë¦¬ë·° í¬ë¡¤ë§ ë° ë¶„ì„")
    print("=" * 60)
    print("ëª©í‘œ: ì•½ 20,000ê°œ ë¦¬ë·° ìˆ˜ì§‘\n")
    
    # Scrape all reviews
    start_time = time.time()
    all_reviews = scrape_all_reviews_selenium()
    
    if not all_reviews:
        print("\në¦¬ë·°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    elapsed_time = time.time() - start_time
    print(f"\ní¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ì†Œìš” ì‹œê°„: {elapsed_time/60:.1f}ë¶„")
    print(f"ìˆ˜ì§‘ëœ ë¦¬ë·° ìˆ˜: {len(all_reviews):,}ê°œ")
    
    # Analyze reviews
    print("\nëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„ì„ ì‹œì‘...")
    df, word_freq, bigram_freq, category_analysis = analyze_large_dataset(all_reviews)
    
    if df is None:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Save analysis results
    save_analysis_report(df, word_freq, bigram_freq, category_analysis)
    
    # Display top results
    print("\n=== ì£¼ìš” ë¶„ì„ ê²°ê³¼ ===")
    print(f"\nğŸ“Š ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ:")
    for i, (word, freq) in enumerate(word_freq.most_common(30), 1):
        if i % 3 == 1:
            print()
        print(f"{i:2d}. {word}: {freq:,}íšŒ", end="  ")
    
    print(f"\n\nğŸ”— ì£¼ìš” ë‹¨ì–´ ì¡°í•©:")
    for i, (bigram, freq) in enumerate(bigram_freq.most_common(15), 1):
        print(f"{i:2d}. {bigram}: {freq}íšŒ")
    
    print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for category, data in category_analysis.items():
        print(f"\n[{category}] - ì´ {data['total_mentions']:,}íšŒ ì–¸ê¸‰")
        top_5 = dict(sorted(data['keywords'].items(), 
                           key=lambda x: x[1], reverse=True)[:5])
        keywords = ', '.join([f"{w}({f:,})" for w, f in top_5.items()])
        print(f"  ì£¼ìš” í‚¤ì›Œë“œ: {keywords}")
    
    # Visualize
    print("\nğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘...")
    visualize_large_dataset(df, word_freq, bigram_freq, category_analysis)
    
    print("\nâœ¨ ì „ì²´ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()