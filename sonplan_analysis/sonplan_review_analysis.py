import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set Korean font
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

def load_and_combine_data():
    """
    Load both CSV files and combine them
    """
    print("CSV íŒŒì¼ ë¡œë“œ ë° ê²°í•© ì¤‘...")
    
    # Load both files
    df1 = pd.read_csv('/Users/jaewansim/Documents/nerdlab/sonplancos_20250718_3321_review1.csv')
    df2 = pd.read_csv('/Users/jaewansim/Documents/nerdlab/sonplancos_20250718_3321_review2.csv')
    
    # Combine dataframes
    df_combined = pd.concat([df1, df2], ignore_index=True)
    
    print(f"íŒŒì¼ 1: {len(df1):,}ê°œ ë¦¬ë·°")
    print(f"íŒŒì¼ 2: {len(df2):,}ê°œ ë¦¬ë·°")
    print(f"ì „ì²´: {len(df_combined):,}ê°œ ë¦¬ë·°")
    
    return df_combined

def preprocess_text(df):
    """
    Preprocess review text for analysis
    """
    print("\ní…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
    
    # Remove empty content
    df = df.dropna(subset=['ë‚´ìš©'])
    df = df[df['ë‚´ìš©'].str.len() > 0]
    
    # Clean text data
    df['cleaned_content'] = df['ë‚´ìš©'].apply(lambda x: str(x))
    
    # Remove duplicate reviews
    original_count = len(df)
    df = df.drop_duplicates(subset=['ë‚´ìš©'])
    print(f"ì¤‘ë³µ ì œê±°: {original_count:,}ê°œ â†’ {len(df):,}ê°œ")
    
    # Parse dates
    df['ì‘ì„±ì¼ì‹œ'] = pd.to_datetime(df['ê²Œì‹œë¬¼ ì‘ì„±ì¼ì‹œ'], errors='coerce')
    
    return df

def extract_keywords(text_series):
    """
    Extract Korean keywords from text
    """
    print("í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    # Combine all text
    all_text = ' '.join(text_series.astype(str))
    
    # Extract Korean words (2-6 characters)
    korean_words = re.findall(r'[ê°€-í£]{2,6}', all_text)
    
    # Stop words for cosmetics reviews
    stop_words = {
        'ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', 'ê°™ì•„ìš”', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì œ', 'ë”', 'ë°', 'ë•Œ', 'ë“±', 'ë°', 
        'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 
        'ê¹Œì§€', 'í•´ìš”', 'í•˜ê³ ', 'í–ˆì–´ìš”', 'ì…ë‹ˆë‹¤', 'ì—ìš”', 'ì˜ˆìš”', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 
        'ë˜ì–´', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆê³ ', 'ì—†ê³ ', 'ê°™ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ëª¨ë“ ', 
        'ê°ê°', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'í†µí•´', 
        'ëŒ€í•´', 'ê´€í•´', 'ë˜í•œ', 'ì—­ì‹œ', 'ì•„ì£¼', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ', 'ì•„ë§ˆ', 
        'ì–¸ì œ', 'ì–´ë””', 'ë¬´ì—‡', 'ëˆ„êµ¬', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–´ëŠ', 'ì–¼ë§ˆ', 'ëª‡', 'ê²Œì‹œë¬¼', 
        'ì œëª©', 'ë‚´ìš©', 'ì‘ì„±ì', 'ì´ë¦„', 'ì•„ì´ë””', 'ì‘ì„±ì¼ì‹œ', 'ì¹´í…Œê³ ë¦¬', 'ê°ì‚¬', 'ê°ì‚¬í•©ë‹ˆë‹¤'
    }
    
    # Filter stop words
    filtered_words = [word for word in korean_words if word not in stop_words]
    
    # Count frequency
    word_freq = Counter(filtered_words)
    
    return word_freq

def analyze_sentiment_keywords(word_freq):
    """
    Categorize keywords by sentiment and themes
    """
    # Define keyword categories for cosmetics
    categories = {
        'ê¸ì •ì  ê°ì •': ['ì¢‹ì•„', 'ì¢‹ì€', 'ì¢‹ë„¤', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ì™„ë²½', 'í›Œë¥­', 'ëŒ€ë°•', 'ì§±', 'êµ¿'],
        'ì œí’ˆ íŠ¹ì„±': ['ì´‰ì´‰', 'ë¶€ë“œëŸ¬', 'ì«€ì«€', 'ê°€ë²¼ìš´', 'ì‚°ëœ»', 'ëˆì ', 'ë¬´ê±°ìš´', 'í…ìŠ¤ì²˜', 'ì œí˜•'],
        'íš¨ê³¼': ['íš¨ê³¼', 'ê°œì„ ', 'ì¢‹ì•„ì¡Œ', 'ë³€í™”', 'ë‹¬ë¼', 'ëŠë‚Œ', 'íƒ„ë ¥', 'ì£¼ë¦„', 'ë³´ìŠµ', 'ìˆ˜ë¶„'],
        'ì‚¬ìš©ì„±': ['ë°œë¦¼', 'í¡ìˆ˜', 'ì‚¬ìš©', 'ë°œë¼', 'ë°”ë¥´ê¸°', 'í´ë°œ', 'ìŠ¤ë©°', 'ë°œë¦¼ì„±'],
        'êµ¬ë§¤/ì¬êµ¬ë§¤': ['êµ¬ë§¤', 'ì¬êµ¬ë§¤', 'ë¦¬í”¼', 'ë˜', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì£¼ë¬¸', 'ìƒ€ì–´', 'ì‚´ê²Œ'],
        'ì¶”ì²œ/ê³µìœ ': ['ì¶”ì²œ', 'ì†Œê°œ', 'ì…ì†Œë¬¸', 'ì—„ë§ˆ', 'ì¹œêµ¬', 'ê°€ì¡±', 'ì£¼ë³€', 'ê°™ì´'],
        'ë¶€ì •ì ': ['ë³„ë¡œ', 'ì•„ì‰¬ì›Œ', 'ì‹¤ë§', 'ì•ˆì¢‹', 'ê·¸ì €ê·¸ë˜', 'ë³´í†µ', 'í ']
    }
    
    category_analysis = {}
    
    for category, keywords in categories.items():
        category_words = {}
        for word, freq in word_freq.items():
            for keyword in keywords:
                if keyword in word:
                    category_words[word] = freq
                    break
        category_analysis[category] = category_words
    
    return category_analysis

def create_wordcloud(word_freq, title="ì›Œë“œí´ë¼ìš°ë“œ"):
    """
    Create word cloud visualization
    """
    print(f"{title} ìƒì„± ì¤‘...")
    
    # Create word cloud
    wordcloud = WordCloud(
        font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
        background_color='white',
        width=1200,
        height=800,
        max_words=150,
        relative_scaling=0.5,
        min_font_size=10,
        colormap='viridis'
    ).generate_from_frequencies(dict(word_freq))
    
    plt.figure(figsize=(12, 8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=20, fontweight='bold', pad=20)
    
    # Save
    filename = f'sonplan_{title.replace(" ", "_")}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    return filename

def create_comprehensive_analysis(df, word_freq, category_analysis):
    """
    Create comprehensive analysis visualizations
    """
    print("\nì¢…í•© ë¶„ì„ ì‹œê°í™” ìƒì„± ì¤‘...")
    
    fig = plt.figure(figsize=(20, 16))
    
    # 1. Top Keywords
    ax1 = plt.subplot(3, 3, 1)
    top_words = dict(word_freq.most_common(20))
    bars = ax1.bar(range(len(top_words)), list(top_words.values()), color='skyblue')
    ax1.set_xticks(range(len(top_words)))
    ax1.set_xticklabels(list(top_words.keys()), rotation=45, ha='right')
    ax1.set_title('ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 20', fontsize=14, fontweight='bold')
    ax1.set_ylabel('ë¹ˆë„ìˆ˜')
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom', fontsize=8)
    
    # 2. Review Length Distribution
    ax2 = plt.subplot(3, 3, 2)
    review_lengths = df['ë‚´ìš©'].str.len()
    ax2.hist(review_lengths, bins=50, color='lightgreen', edgecolor='black', alpha=0.7)
    ax2.set_title('ë¦¬ë·° ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')
    ax2.set_xlabel('ê¸€ì ìˆ˜')
    ax2.set_ylabel('ë¦¬ë·° ìˆ˜')
    ax2.axvline(review_lengths.mean(), color='red', linestyle='dashed', linewidth=2,
                label=f'í‰ê· : {review_lengths.mean():.0f}ì')
    ax2.legend()
    
    # 3. Monthly Review Trend
    ax3 = plt.subplot(3, 3, 3)
    if df['ì‘ì„±ì¼ì‹œ'].notna().any():
        df['ì›”'] = df['ì‘ì„±ì¼ì‹œ'].dt.to_period('M')
        monthly_counts = df.groupby('ì›”').size()
        monthly_counts.plot(ax=ax3, kind='line', marker='o', linewidth=2, markersize=6)
        ax3.set_title('ì›”ë³„ ë¦¬ë·° ì¶”ì´', fontsize=14, fontweight='bold')
        ax3.set_xlabel('ì›”')
        ax3.set_ylabel('ë¦¬ë·° ìˆ˜')
        ax3.tick_params(axis='x', rotation=45)
    
    # 4-8. Category Analysis
    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']
    
    for idx, (category, words) in enumerate(list(category_analysis.items())[:5]):
        ax = plt.subplot(3, 3, 4 + idx)
        if words:
            top_category_words = dict(sorted(words.items(), key=lambda x: x[1], reverse=True)[:10])
            if top_category_words:
                bars = ax.bar(range(len(top_category_words)), list(top_category_words.values()),
                             color=colors[idx % len(colors)])
                ax.set_xticks(range(len(top_category_words)))
                ax.set_xticklabels(list(top_category_words.keys()), rotation=45, ha='right')
                ax.set_title(f'{category} ê´€ë ¨ í‚¤ì›Œë“œ', fontsize=12, fontweight='bold')
                ax.set_ylabel('ë¹ˆë„ìˆ˜')
    
    # 9. Summary Statistics
    ax9 = plt.subplot(3, 3, 9)
    ax9.axis('off')
    
    total_reviews = len(df)
    unique_keywords = len(word_freq)
    avg_length = review_lengths.mean()
    
    # Calculate sentiment ratio
    positive_words = len(category_analysis.get('ê¸ì •ì  ê°ì •', {}))
    negative_words = len(category_analysis.get('ë¶€ì •ì ', {}))
    
    summary_text = f"""
    ğŸ“Š ì¬í”Œëœ ë¦¬ë·° ë¶„ì„ ìš”ì•½
    
    ì´ ë¦¬ë·° ìˆ˜: {total_reviews:,}ê°œ
    í‰ê·  ë¦¬ë·° ê¸¸ì´: {avg_length:.0f}ì
    ì¶”ì¶œëœ ê³ ìœ  í‚¤ì›Œë“œ: {unique_keywords:,}ê°œ
    
    ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­:
    â€¢ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ:
      {', '.join(list(top_words.keys())[:5])}
    
    â€¢ ê¸ì • í‚¤ì›Œë“œ: {positive_words}ê°œ
    â€¢ ë¶€ì • í‚¤ì›Œë“œ: {negative_words}ê°œ
    
    â€¢ ë¦¬ë·° ê¸°ê°„: {df['ì‘ì„±ì¼ì‹œ'].min().strftime('%Y-%m') if df['ì‘ì„±ì¼ì‹œ'].notna().any() else 'N/A'} ~ 
      {df['ì‘ì„±ì¼ì‹œ'].max().strftime('%Y-%m') if df['ì‘ì„±ì¼ì‹œ'].notna().any() else 'N/A'}
    """
    
    ax9.text(0.05, 0.95, summary_text, transform=ax9.transAxes,
             fontsize=11, verticalalignment='top', fontfamily='AppleGothic')
    
    plt.tight_layout()
    plt.savefig('sonplan_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def create_detailed_report(df, word_freq, category_analysis):
    """
    Create detailed text report
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    with open(f'sonplan_analysis_report_{timestamp}.txt', 'w', encoding='utf-8') as f:
        f.write("ì¬í”Œëœ ì œí’ˆ ë¦¬ë·° ë¶„ì„ ë³´ê³ ì„œ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
        f.write(f"ì´ ë¦¬ë·° ìˆ˜: {len(df):,}ê°œ\n\n")
        
        # Basic statistics
        f.write("=== ê¸°ë³¸ í†µê³„ ===\n")
        f.write(f"í‰ê·  ë¦¬ë·° ê¸¸ì´: {df['ë‚´ìš©'].str.len().mean():.0f}ì\n")
        f.write(f"ìµœëŒ€ ë¦¬ë·° ê¸¸ì´: {df['ë‚´ìš©'].str.len().max()}ì\n")
        f.write(f"ìµœì†Œ ë¦¬ë·° ê¸¸ì´: {df['ë‚´ìš©'].str.len().min()}ì\n\n")
        
        # Top keywords
        f.write("=== ìƒìœ„ 50ê°œ í‚¤ì›Œë“œ ===\n")
        for i, (word, freq) in enumerate(word_freq.most_common(50), 1):
            f.write(f"{i:2d}. {word}: {freq:,}íšŒ\n")
        
        # Category analysis
        f.write("\n=== ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ===\n")
        for category, words in category_analysis.items():
            f.write(f"\n[{category}] - ì´ {sum(words.values()):,}íšŒ ì–¸ê¸‰\n")
            top_words = dict(sorted(words.items(), key=lambda x: x[1], reverse=True)[:10])
            for word, freq in top_words.items():
                f.write(f"  - {word}: {freq:,}íšŒ\n")
        
        # Sample reviews
        f.write("\n=== ë¦¬ë·° ìƒ˜í”Œ ===\n")
        sample_reviews = df.sample(n=min(20, len(df)), random_state=42)
        for i, (_, row) in enumerate(sample_reviews.iterrows(), 1):
            f.write(f"\n{i}. [{row['ì‘ì„±ì ì´ë¦„']}] {row['ê²Œì‹œë¬¼ ì‘ì„±ì¼ì‹œ']}\n")
            f.write(f"   {row['ë‚´ìš©'][:100]}{'...' if len(row['ë‚´ìš©']) > 100 else ''}\n")
    
    return f'sonplan_analysis_report_{timestamp}.txt'

def main():
    print("ì¬í”Œëœ ë¦¬ë·° ë°ì´í„° ë¶„ì„ ì‹œì‘")
    print("=" * 50)
    
    # Load and preprocess data
    df = load_and_combine_data()
    df = preprocess_text(df)
    
    # Extract keywords
    word_freq = extract_keywords(df['ë‚´ìš©'])
    
    # Analyze categories
    category_analysis = analyze_sentiment_keywords(word_freq)
    
    # Create visualizations
    print("\n=== ì‹œê°í™” ìƒì„± ===")
    
    # 1. Main word cloud
    create_wordcloud(word_freq, "ì¬í”Œëœ ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ")
    
    # 2. Positive keywords word cloud
    positive_words = category_analysis.get('ê¸ì •ì  ê°ì •', {})
    if positive_words:
        create_wordcloud(positive_words, "ê¸ì •ì  í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
    
    # 3. Product characteristics word cloud
    product_words = category_analysis.get('ì œí’ˆ íŠ¹ì„±', {})
    if product_words:
        create_wordcloud(product_words, "ì œí’ˆ íŠ¹ì„± í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
    
    # 4. Comprehensive analysis
    create_comprehensive_analysis(df, word_freq, category_analysis)
    
    # Print results
    print("\n=== ì£¼ìš” ë¶„ì„ ê²°ê³¼ ===")
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"- ì´ ë¦¬ë·° ìˆ˜: {len(df):,}ê°œ")
    print(f"- í‰ê·  ë¦¬ë·° ê¸¸ì´: {df['ë‚´ìš©'].str.len().mean():.0f}ì")
    print(f"- ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(word_freq):,}ê°œ")
    
    print(f"\nğŸ”¥ ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ:")
    for i, (word, freq) in enumerate(word_freq.most_common(20), 1):
        if i % 4 == 1 and i > 1:
            print()
        print(f"{i:2d}.{word}({freq:,})", end="  ")
    
    print(f"\n\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for category, words in category_analysis.items():
        if words:
            total_mentions = sum(words.values())
            top_word = max(words.items(), key=lambda x: x[1])
            print(f"- {category}: {total_mentions:,}íšŒ (ì£¼ìš”: {top_word[0]})")
    
    # Save detailed report
    report_file = create_detailed_report(df, word_freq, category_analysis)
    print(f"\nğŸ“ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")
    
    # Save processed data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Keywords CSV
    keywords_df = pd.DataFrame([
        {'í‚¤ì›Œë“œ': word, 'ë¹ˆë„': freq, 'ìˆœìœ„': i+1}
        for i, (word, freq) in enumerate(word_freq.most_common(200))
    ])
    keywords_df.to_csv(f'sonplan_keywords_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    # Category analysis CSV
    category_data = []
    for category, words in category_analysis.items():
        for word, freq in words.items():
            category_data.append({
                'ì¹´í…Œê³ ë¦¬': category,
                'í‚¤ì›Œë“œ': word,
                'ë¹ˆë„': freq
            })
    
    if category_data:
        category_df = pd.DataFrame(category_data)
        category_df = category_df.sort_values(['ì¹´í…Œê³ ë¦¬', 'ë¹ˆë„'], ascending=[True, False])
        category_df.to_csv(f'sonplan_categories_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    print(f"\nğŸ’¾ ê²°ê³¼ íŒŒì¼:")
    print(f"- í‚¤ì›Œë“œ ë¶„ì„: sonplan_keywords_{timestamp}.csv")
    print(f"- ì¹´í…Œê³ ë¦¬ ë¶„ì„: sonplan_categories_{timestamp}.csv")
    print(f"- ì›Œë“œí´ë¼ìš°ë“œ: sonplan_*ì›Œë“œí´ë¼ìš°ë“œ*.png")
    print(f"- ì¢…í•© ë¶„ì„: sonplan_comprehensive_analysis.png")
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()