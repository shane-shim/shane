import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from collections import Counter
import re
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set Korean font
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

def load_and_clean_data():
    """
    Load both CSV files, combine them, and remove NaverPay reviews
    """
    print("CSV íŒŒì¼ ë¡œë“œ ë° ì •ì œ ì¤‘...")
    
    # Load both files
    df1 = pd.read_csv('/Users/jaewansim/Documents/nerdlab/sonplancos_20250718_3321_review1.csv')
    df2 = pd.read_csv('/Users/jaewansim/Documents/nerdlab/sonplancos_20250718_3321_review2.csv')
    
    print(f"íŒŒì¼ 1 ì›ë³¸: {len(df1):,}ê°œ ë¦¬ë·°")
    print(f"íŒŒì¼ 2 ì›ë³¸: {len(df2):,}ê°œ ë¦¬ë·°")
    
    # Combine dataframes
    df_combined = pd.concat([df1, df2], ignore_index=True)
    print(f"ì „ì²´ ì›ë³¸: {len(df_combined):,}ê°œ ë¦¬ë·°")
    
    # Remove NaverPay reviews
    # Look for patterns indicating NaverPay automated reviews
    naverpay_patterns = [
        'ë„¤ì´ë²„í˜ì´ êµ¬ë§¤í‰',
        'ë„¤ì´ë²„ í˜ì´ êµ¬ë§¤í‰',
        'ë„¤ì´ë²„í˜ì´',
        'ë„¤ì´ë²„ í˜ì´',
        'êµ¬ë§¤í‰ ë“±ë¡',
        'ìë™ ë“±ë¡',
        'ë„¤ì´ë²„ì‡¼í•‘',
        'NAVER',
        'ë„¤ì´ë²„',
        'ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´'
    ]
    
    # Check for NaverPay patterns in content
    original_count = len(df_combined)
    
    for pattern in naverpay_patterns:
        df_combined = df_combined[~df_combined['ë‚´ìš©'].str.contains(pattern, na=False)]
    
    # Also check for very short generic reviews (likely automated)
    df_combined = df_combined[df_combined['ë‚´ìš©'].str.len() > 5]
    
    # Remove reviews with only emojis or very repetitive content
    df_combined = df_combined[~df_combined['ë‚´ìš©'].str.match(r'^[^\wê°€-í£]*$', na=False)]
    
    # Check for duplicate content (common in automated reviews)
    df_combined = df_combined.drop_duplicates(subset=['ë‚´ìš©'])
    
    print(f"ì •ì œ í›„: {len(df_combined):,}ê°œ ë¦¬ë·°")
    print(f"ì œê±°ëœ ë¦¬ë·°: {original_count - len(df_combined):,}ê°œ")
    
    # Clean and preprocess
    df_combined = df_combined.dropna(subset=['ë‚´ìš©'])
    df_combined['ì‘ì„±ì¼ì‹œ'] = pd.to_datetime(df_combined['ê²Œì‹œë¬¼ ì‘ì„±ì¼ì‹œ'], errors='coerce')
    
    return df_combined

def extract_meaningful_keywords(text_series):
    """
    Extract meaningful Korean keywords from text
    """
    print("ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    
    # Combine all text
    all_text = ' '.join(text_series.astype(str))
    
    # Extract Korean words (2-8 characters for more meaningful words)
    korean_words = re.findall(r'[ê°€-í£]{2,8}', all_text)
    
    # Comprehensive stop words
    stop_words = {
        # Common words
        'ìˆì–´ìš”', 'ìˆìŠµë‹ˆë‹¤', 'ê°™ì•„ìš”', 'ê²ƒ', 'ìˆ˜', 'ì €', 'ì œ', 'ë”', 'ë°', 'ë•Œ', 'ë“±', 'ë°', 
        'ì´', 'ê·¸', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ê°€', 'ì€', 'ëŠ”', 'ë„', 'ë¡œ', 'ìœ¼ë¡œ', 'ë§Œ', 
        'ê¹Œì§€', 'í•´ìš”', 'í•˜ê³ ', 'í–ˆì–´ìš”', 'ì…ë‹ˆë‹¤', 'ì—ìš”', 'ì˜ˆìš”', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 
        'ë˜ì–´', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ìˆê³ ', 'ì—†ê³ ', 'ê°™ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ëª¨ë“ ', 
        'ê°ê°', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ë•Œë¬¸ì—', 'ìœ„í•´', 'í†µí•´', 
        'ëŒ€í•´', 'ê´€í•´', 'ë˜í•œ', 'ì—­ì‹œ', 'ì•„ì£¼', 'ë§¤ìš°', 'ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ', 'ì•„ë§ˆ', 
        'ì–¸ì œ', 'ì–´ë””', 'ë¬´ì—‡', 'ëˆ„êµ¬', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–´ëŠ', 'ì–¼ë§ˆ', 'ëª‡',
        # Meta words
        'ê²Œì‹œë¬¼', 'ì œëª©', 'ë‚´ìš©', 'ì‘ì„±ì', 'ì´ë¦„', 'ì•„ì´ë””', 'ì‘ì„±ì¼ì‹œ', 'ì¹´í…Œê³ ë¦¬', 
        'ê°ì‚¬', 'ê°ì‚¬í•©ë‹ˆë‹¤', 'ê³ ë§™ìŠµë‹ˆë‹¤', 'ì•ˆë…•í•˜ì„¸ìš”', 'ì•ˆë…•íˆ', 'ì—¬ëŸ¬ë¶„', 'ëª¨ë‘', 'ë‹¤ë“¤',
        # Generic phrases
        'ìƒê°', 'ë§ˆìŒ', 'ê¸°ë¶„', 'ëŠë‚Œ', 'ì •ë„', 'ìƒíƒœ', 'ê²½ìš°', 'ë°©ë²•', 'ì‹œê°„', 'ë‹¤ìŒ', 
        'ì´ë²ˆ', 'ì§€ê¸ˆ', 'ê·¸ë•Œ', 'ìš”ì¦˜', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì–´ì œ', 'ì²˜ìŒ', 'ë§ˆì§€ë§‰', 'ê³„ì†',
        # Filler words
        'ê·¸ëƒ¥', 'ì¡°ê¸ˆ', 'ì•½ê°„', 'ì‚´ì§', 'ì¢€', 'ë§ì´', 'ì™„ì „', 'ì§„ì§œ', 'ì •ë§', 'ì—„ì²­', 
        'ë˜ê²Œ', 'ê½¤', 'ìƒë‹¹íˆ', 'ë³´í†µ', 'ì¼ë°˜', 'í‰ì†Œ', 'í•­ìƒ', 'ëŠ˜', 'ìì£¼', 'ê°€ë”'
    }
    
    # Filter stop words and short words
    filtered_words = [word for word in korean_words if word not in stop_words and len(word) >= 2]
    
    # Count frequency
    word_freq = Counter(filtered_words)
    
    # Remove words that appear only once (likely typos or very specific terms)
    word_freq = Counter({word: freq for word, freq in word_freq.items() if freq > 1})
    
    return word_freq

def analyze_product_themes(word_freq):
    """
    Categorize keywords by product themes and sentiment
    """
    print("ì œí’ˆ í…Œë§ˆë³„ í‚¤ì›Œë“œ ë¶„ì„ ì¤‘...")
    
    # Define detailed categories for cosmetics analysis
    theme_categories = {
        'ë§Œì¡±ë„': {
            'ê¸ì •': ['ì¢‹ì•„', 'ì¢‹ì€', 'ì¢‹ë„¤', 'ì¢‹ë‹¤', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ì™„ë²½', 'í›Œë¥­', 'ëŒ€ë°•', 'ì§±', 'êµ¿', 'ë² ìŠ¤íŠ¸', 'ì™„ì „'],
            'ë¶€ì •': ['ë³„ë¡œ', 'ì•„ì‰¬ì›Œ', 'ì‹¤ë§', 'ì•ˆì¢‹', 'ê·¸ì €ê·¸ë˜', 'ë³´í†µ', 'í ', 'ë³„ë¡œë„¤', 'ì•„ì‰½']
        },
        'ì œí’ˆíŠ¹ì„±': {
            'í…ìŠ¤ì²˜': ['ì´‰ì´‰', 'ë¶€ë“œëŸ¬', 'ì«€ì«€', 'ê°€ë²¼ìš´', 'ì‚°ëœ»', 'ëˆì ', 'ë¬´ê±°ìš´', 'í…ìŠ¤ì²˜', 'ì œí˜•', 'ë°œë¦¼', 'í¡ìˆ˜'],
            'í–¥': ['í–¥', 'ëƒ„ìƒˆ', 'í–¥ê¸°', 'ì‹œì¹´í–¥', 'ë¬´í–¥', 'í–¥ì´'],
            'ìš©ëŸ‰': ['ìš©ëŸ‰', 'ë§ì´', 'ì ë‹¹', 'í¬ê¸°']
        },
        'íš¨ê³¼': {
            'ë³´ìŠµ': ['ë³´ìŠµ', 'ìˆ˜ë¶„', 'ì´‰ì´‰', 'ê±´ì¡°', 'ë‹¹ê¹€'],
            'ê°œì„ ': ['íš¨ê³¼', 'ê°œì„ ', 'ì¢‹ì•„ì¡Œ', 'ë³€í™”', 'ë‹¬ë¼', 'íƒ„ë ¥', 'ì£¼ë¦„'],
            'ì•ˆì „ì„±': ['ìê·¹', 'ìˆœí•œ', 'ë¯¼ê°', 'ì•ŒëŸ¬ì§€', 'íŠ¸ëŸ¬ë¸”', 'ë”°ê°€ì›€']
        },
        'ì‚¬ìš©ê²½í—˜': {
            'ì‚¬ìš©ì„±': ['ì‚¬ìš©', 'ë°œë¼', 'ë°”ë¥´ê¸°', 'í´ë°œ', 'ìŠ¤ë©°', 'ë°œë¦¼ì„±', 'ì„¸ìˆ˜', 'ì”»ê³ '],
            'ì§€ì†ì„±': ['ì§€ì†', 'ì˜¤ë˜', 'ê³„ì†', 'í•˜ë£¨', 'ì´í‹€', 'ë©°ì¹ ']
        },
        'êµ¬ë§¤í–‰ë™': {
            'êµ¬ë§¤': ['êµ¬ë§¤', 'ìƒ€ì–´', 'ì‚¬ëŸ¬', 'ì£¼ë¬¸', 'êµ¬ì…'],
            'ì¬êµ¬ë§¤': ['ì¬êµ¬ë§¤', 'ë¦¬í”¼', 'ë˜', 'ë‹¤ì‹œ', 'ê³„ì†', 'ëª‡ë²ˆì§¸'],
            'ì¶”ì²œ': ['ì¶”ì²œ', 'ì†Œê°œ', 'ì…ì†Œë¬¸', 'ì—„ë§ˆ', 'ì¹œêµ¬', 'ê°€ì¡±', 'ì£¼ë³€', 'ê°™ì´', 'ì´ëª¨']
        }
    }
    
    # Analyze themes
    theme_analysis = {}
    
    for main_theme, sub_themes in theme_categories.items():
        theme_analysis[main_theme] = {}
        
        for sub_theme, keywords in sub_themes.items():
            matched_words = {}
            
            for word, freq in word_freq.items():
                for keyword in keywords:
                    if keyword in word:
                        matched_words[word] = freq
                        break
            
            if matched_words:
                theme_analysis[main_theme][sub_theme] = matched_words
    
    return theme_analysis

def create_enhanced_wordcloud(word_freq, title="ì›Œë“œí´ë¼ìš°ë“œ", colormap='viridis'):
    """
    Create enhanced word cloud visualization
    """
    print(f"{title} ìƒì„± ì¤‘...")
    
    if not word_freq:
        print(f"No words found for {title}")
        return None
    
    # Create word cloud with better settings
    wordcloud = WordCloud(
        font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
        background_color='white',
        width=1400,
        height=900,
        max_words=100,
        relative_scaling=0.5,
        min_font_size=12,
        max_font_size=80,
        colormap=colormap,
        prefer_horizontal=0.7
    ).generate_from_frequencies(dict(word_freq))
    
    plt.figure(figsize=(14, 9))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title, fontsize=22, fontweight='bold', pad=30)
    
    # Save with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'sonplan_{title.replace(" ", "_")}_{timestamp}.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    return filename

def create_comprehensive_dashboard(df, word_freq, theme_analysis):
    """
    Create comprehensive analysis dashboard
    """
    print("ì¢…í•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
    
    fig = plt.figure(figsize=(24, 18))
    
    # 1. Top Keywords (larger)
    ax1 = plt.subplot(3, 4, (1, 2))
    top_words = dict(word_freq.most_common(25))
    bars = ax1.bar(range(len(top_words)), list(top_words.values()), 
                   color=plt.cm.Set3(np.linspace(0, 1, len(top_words))))
    ax1.set_xticks(range(len(top_words)))
    ax1.set_xticklabels(list(top_words.keys()), rotation=45, ha='right')
    ax1.set_title('ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ TOP 25', fontsize=16, fontweight='bold')
    ax1.set_ylabel('ë¹ˆë„ìˆ˜')
    
    # Add value labels
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom', fontsize=9)
    
    # 2. Review Length Distribution
    ax2 = plt.subplot(3, 4, 3)
    review_lengths = df['ë‚´ìš©'].str.len()
    ax2.hist(review_lengths, bins=30, color='lightblue', edgecolor='black', alpha=0.7)
    ax2.set_title('ë¦¬ë·° ê¸¸ì´ ë¶„í¬', fontsize=14, fontweight='bold')
    ax2.set_xlabel('ê¸€ì ìˆ˜')
    ax2.set_ylabel('ë¦¬ë·° ìˆ˜')
    ax2.axvline(review_lengths.mean(), color='red', linestyle='dashed', linewidth=2)
    
    # 3. Monthly Review Trend
    ax3 = plt.subplot(3, 4, 4)
    if df['ì‘ì„±ì¼ì‹œ'].notna().any():
        df['ë…„ì›”'] = df['ì‘ì„±ì¼ì‹œ'].dt.to_period('M')
        monthly_counts = df.groupby('ë…„ì›”').size()
        monthly_counts.plot(ax=ax3, kind='line', marker='o', linewidth=2, markersize=6, color='green')
        ax3.set_title('ì›”ë³„ ë¦¬ë·° ì¶”ì´', fontsize=14, fontweight='bold')
        ax3.set_xlabel('ì›”')
        ax3.set_ylabel('ë¦¬ë·° ìˆ˜')
        ax3.tick_params(axis='x', rotation=45)
    
    # 4-8. Theme Analysis
    theme_colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc', '#c2c2f0']
    plot_idx = 5
    
    for theme_idx, (main_theme, sub_themes) in enumerate(theme_analysis.items()):
        if plot_idx > 12:  # Limit to available subplot spaces
            break
            
        ax = plt.subplot(3, 4, plot_idx)
        
        # Aggregate all words from sub-themes
        all_theme_words = {}
        for sub_theme, words in sub_themes.items():
            all_theme_words.update(words)
        
        if all_theme_words:
            top_theme_words = dict(sorted(all_theme_words.items(), 
                                         key=lambda x: x[1], reverse=True)[:10])
            
            bars = ax.bar(range(len(top_theme_words)), list(top_theme_words.values()),
                         color=theme_colors[theme_idx % len(theme_colors)])
            ax.set_xticks(range(len(top_theme_words)))
            ax.set_xticklabels(list(top_theme_words.keys()), rotation=45, ha='right')
            ax.set_title(f'{main_theme} ê´€ë ¨ í‚¤ì›Œë“œ', fontsize=12, fontweight='bold')
            ax.set_ylabel('ë¹ˆë„ìˆ˜')
        
        plot_idx += 1
    
    plt.tight_layout()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    plt.savefig(f'sonplan_comprehensive_dashboard_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.show()

def generate_insights(df, word_freq, theme_analysis):
    """
    Generate actionable insights from the analysis
    """
    print("\nğŸ” ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘...")
    
    insights = []
    
    # 1. Overall sentiment analysis
    positive_themes = theme_analysis.get('ë§Œì¡±ë„', {}).get('ê¸ì •', {})
    negative_themes = theme_analysis.get('ë§Œì¡±ë„', {}).get('ë¶€ì •', {})
    
    positive_count = sum(positive_themes.values()) if positive_themes else 0
    negative_count = sum(negative_themes.values()) if negative_themes else 0
    
    sentiment_ratio = positive_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else 0
    
    insights.append({
        'category': 'ê³ ê° ë§Œì¡±ë„',
        'finding': f'ê¸ì •ì  ì–¸ê¸‰ì´ {sentiment_ratio:.1%}ë¡œ ì••ë„ì ìœ¼ë¡œ ë§ìŒ',
        'implication': 'ì „ë°˜ì ì¸ ê³ ê° ë§Œì¡±ë„ê°€ ë§¤ìš° ë†’ìŒ',
        'action': 'í˜„ì¬ ì œí’ˆ í’ˆì§ˆ ìœ ì§€ ë° ë§ˆì¼€íŒ… í¬ì¸íŠ¸ë¡œ í™œìš©'
    })
    
    # 2. Top keywords analysis
    top_5_keywords = list(word_freq.most_common(5))
    most_mentioned = top_5_keywords[0][0] if top_5_keywords else "ì—†ìŒ"
    
    insights.append({
        'category': 'í•µì‹¬ í‚¤ì›Œë“œ',
        'finding': f'ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œëŠ” "{most_mentioned}"',
        'implication': 'ì´ í‚¤ì›Œë“œê°€ ê³ ê°ë“¤ì˜ ì£¼ìš” ê´€ì‹¬ì‚¬',
        'action': 'í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë§ˆì¼€íŒ… ë©”ì‹œì§€ ê°•í™”'
    })
    
    # 3. Product characteristics analysis
    texture_words = theme_analysis.get('ì œí’ˆíŠ¹ì„±', {}).get('í…ìŠ¤ì²˜', {})
    if texture_words:
        top_texture = max(texture_words.items(), key=lambda x: x[1])
        insights.append({
            'category': 'ì œí’ˆ íŠ¹ì„±',
            'finding': f'í…ìŠ¤ì²˜ ê´€ë ¨í•´ì„œëŠ” "{top_texture[0]}"ì´ ê°€ì¥ ë§ì´ ì–¸ê¸‰ë¨',
            'implication': 'ê³ ê°ë“¤ì´ ì¸ì‹í•˜ëŠ” ì£¼ìš” ì œí’ˆ íŠ¹ì„±',
            'action': 'ì œí’ˆ ì„¤ëª… ë° ê´‘ê³ ì—ì„œ í•´ë‹¹ íŠ¹ì„± ë¶€ê°'
        })
    
    # 4. Purchase behavior analysis
    repurchase_words = theme_analysis.get('êµ¬ë§¤í–‰ë™', {}).get('ì¬êµ¬ë§¤', {})
    if repurchase_words:
        repurchase_mentions = sum(repurchase_words.values())
        insights.append({
            'category': 'ì¬êµ¬ë§¤ ì˜ë„',
            'finding': f'ì¬êµ¬ë§¤ ê´€ë ¨ ì–¸ê¸‰ì´ {repurchase_mentions}íšŒ ë‚˜íƒ€ë‚¨',
            'implication': 'ê³ ê° ì¶©ì„±ë„ê°€ ë†’ê³  ì¬êµ¬ë§¤ ì˜ë„ê°€ ê°•í•¨',
            'action': 'ë¦¬í”¼í„° ê³ ê° ëŒ€ìƒ íŠ¹ë³„ í˜œíƒ ë° í”„ë¡œëª¨ì…˜ ê¸°íš'
        })
    
    # 5. Word-of-mouth analysis
    recommendation_words = theme_analysis.get('êµ¬ë§¤í–‰ë™', {}).get('ì¶”ì²œ', {})
    if recommendation_words:
        wom_mentions = sum(recommendation_words.values())
        insights.append({
            'category': 'ì…ì†Œë¬¸ íš¨ê³¼',
            'finding': f'ì¶”ì²œ/ì…ì†Œë¬¸ ê´€ë ¨ ì–¸ê¸‰ì´ {wom_mentions}íšŒ ë‚˜íƒ€ë‚¨',
            'implication': 'ìì—°ìŠ¤ëŸ¬ìš´ ì…ì†Œë¬¸ì´ í™œë°œíˆ ì¼ì–´ë‚˜ê³  ìˆìŒ',
            'action': 'ë¦¬ë·° ì´ë²¤íŠ¸ ë° ì¶”ì²œ ë¦¬ì›Œë“œ í”„ë¡œê·¸ë¨ ìš´ì˜'
        })
    
    # 6. Usage experience analysis
    usage_words = theme_analysis.get('ì‚¬ìš©ê²½í—˜', {}).get('ì‚¬ìš©ì„±', {})
    if usage_words:
        top_usage = max(usage_words.items(), key=lambda x: x[1])
        insights.append({
            'category': 'ì‚¬ìš© ê²½í—˜',
            'finding': f'ì‚¬ìš© ê´€ë ¨í•´ì„œëŠ” "{top_usage[0]}"ì´ ì£¼ìš” í‚¤ì›Œë“œ',
            'implication': 'ê³ ê°ë“¤ì˜ ì‹¤ì œ ì‚¬ìš© ê²½í—˜ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸',
            'action': 'ì‚¬ìš©ë²• ê°€ì´ë“œ ë° ì‚¬ìš© íŒ ì½˜í…ì¸  ì œì‘'
        })
    
    # 7. Review volume analysis
    total_reviews = len(df)
    if df['ì‘ì„±ì¼ì‹œ'].notna().any():
        review_period = (df['ì‘ì„±ì¼ì‹œ'].max() - df['ì‘ì„±ì¼ì‹œ'].min()).days
        daily_avg = total_reviews / review_period if review_period > 0 else 0
        
        insights.append({
            'category': 'ë¦¬ë·° í™œë™ëŸ‰',
            'finding': f'ì¼í‰ê·  {daily_avg:.1f}ê°œì˜ ë¦¬ë·°ê°€ ì‘ì„±ë¨',
            'implication': 'í™œë°œí•œ ê³ ê° ì°¸ì—¬ë„',
            'action': 'ì§€ì†ì ì¸ ê³ ê° ì†Œí†µ ë° í”¼ë“œë°± ê´€ë¦¬ í•„ìš”'
        })
    
    return insights

def create_insight_report(insights, df, word_freq, theme_analysis):
    """
    Create comprehensive insight report
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'sonplan_insights_report_{timestamp}.txt'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("ì¬í”Œëœ ì œí’ˆ ë¦¬ë·° ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ë³´ê³ ì„œ\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„')}\n")
        f.write(f"ë¶„ì„ ëŒ€ìƒ: ì •ì œëœ ë¦¬ë·° {len(df):,}ê°œ\n\n")
        
        # Executive Summary
        f.write("ğŸ“ˆ EXECUTIVE SUMMARY\n")
        f.write("-" * 30 + "\n")
        f.write(f"â€¢ ì´ ë¦¬ë·° ìˆ˜: {len(df):,}ê°œ\n")
        f.write(f"â€¢ í‰ê·  ë¦¬ë·° ê¸¸ì´: {df['ë‚´ìš©'].str.len().mean():.0f}ì\n")
        f.write(f"â€¢ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(word_freq):,}ê°œ\n")
        f.write(f"â€¢ ì£¼ìš” í‚¤ì›Œë“œ: {', '.join([word for word, _ in word_freq.most_common(5)])}\n\n")
        
        # Key Insights
        f.write("ğŸ” KEY INSIGHTS\n")
        f.write("-" * 30 + "\n")
        for i, insight in enumerate(insights, 1):
            f.write(f"{i}. {insight['category']}\n")
            f.write(f"   ë°œê²¬ì‚¬í•­: {insight['finding']}\n")
            f.write(f"   ì‹œì‚¬ì : {insight['implication']}\n")
            f.write(f"   ì•¡ì…˜ ì•„ì´í…œ: {insight['action']}\n\n")
        
        # Detailed Analysis
        f.write("ğŸ“Š DETAILED ANALYSIS\n")
        f.write("-" * 30 + "\n\n")
        
        # Top Keywords
        f.write("ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ:\n")
        for i, (word, freq) in enumerate(word_freq.most_common(30), 1):
            f.write(f"{i:2d}. {word}: {freq:,}íšŒ\n")
        
        # Theme Analysis
        f.write("\ní…Œë§ˆë³„ ë¶„ì„:\n")
        for main_theme, sub_themes in theme_analysis.items():
            f.write(f"\n[{main_theme}]\n")
            for sub_theme, words in sub_themes.items():
                if words:
                    f.write(f"  {sub_theme}: {sum(words.values()):,}íšŒ ì–¸ê¸‰\n")
                    top_words = dict(sorted(words.items(), key=lambda x: x[1], reverse=True)[:5])
                    f.write(f"    ì£¼ìš” í‚¤ì›Œë“œ: {', '.join([f'{w}({c})' for w, c in top_words.items()])}\n")
        
        # Recommendations
        f.write("\nğŸ¯ RECOMMENDATIONS\n")
        f.write("-" * 30 + "\n")
        f.write("1. ë§ˆì¼€íŒ… ì „ëµ:\n")
        f.write("   - ê³ ê° ë§Œì¡±ë„ê°€ ë†’ìœ¼ë¯€ë¡œ testimonial ë§ˆì¼€íŒ… í™œìš©\n")
        f.write("   - ì£¼ìš” í‚¤ì›Œë“œë¥¼ í™œìš©í•œ SEO ìµœì í™”\n\n")
        
        f.write("2. ì œí’ˆ ê°œë°œ:\n")
        f.write("   - í˜„ì¬ ì œí’ˆ í’ˆì§ˆ ìœ ì§€ê°€ ìµœìš°ì„ \n")
        f.write("   - ê³ ê°ì´ ì–¸ê¸‰í•˜ëŠ” ì£¼ìš” íŠ¹ì„± ê°•í™”\n\n")
        
        f.write("3. ê³ ê° ê´€ë¦¬:\n")
        f.write("   - ë¦¬ë·° ì‘ì„± ê³ ê° ëŒ€ìƒ ë¦¬ì›Œë“œ í”„ë¡œê·¸ë¨\n")
        f.write("   - ì¬êµ¬ë§¤ ê³ ê° ëŒ€ìƒ íŠ¹ë³„ í˜œíƒ\n\n")
        
        f.write("4. ì½˜í…ì¸  ì „ëµ:\n")
        f.write("   - ê³ ê° ë¦¬ë·° ê¸°ë°˜ ì‚¬ìš©ë²• ê°€ì´ë“œ ì œì‘\n")
        f.write("   - ì…ì†Œë¬¸ íš¨ê³¼ë¥¼ í™œìš©í•œ ì†Œì…œ ë§ˆì¼€íŒ…\n")
    
    return filename

def main():
    print("ì¬í”Œëœ ë¦¬ë·° ë°ì´í„° ì •ì œ ë° ë¶„ì„ ì‹œì‘")
    print("=" * 60)
    
    # 1. Load and clean data
    df = load_and_clean_data()
    
    # 2. Extract meaningful keywords
    word_freq = extract_meaningful_keywords(df['ë‚´ìš©'])
    
    # 3. Analyze themes
    theme_analysis = analyze_product_themes(word_freq)
    
    # 4. Create visualizations
    print("\n=== ì‹œê°í™” ìƒì„± ===")
    
    # Main word cloud
    create_enhanced_wordcloud(word_freq, "ì¬í”Œëœ ë¦¬ë·° ì „ì²´ ì›Œë“œí´ë¼ìš°ë“œ", 'viridis')
    
    # Positive sentiment word cloud
    positive_words = theme_analysis.get('ë§Œì¡±ë„', {}).get('ê¸ì •', {})
    if positive_words:
        create_enhanced_wordcloud(positive_words, "ê¸ì •ì  í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ", 'Greens')
    
    # Product characteristics word cloud
    product_words = {}
    for sub_theme, words in theme_analysis.get('ì œí’ˆíŠ¹ì„±', {}).items():
        product_words.update(words)
    if product_words:
        create_enhanced_wordcloud(product_words, "ì œí’ˆ íŠ¹ì„± í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ", 'Blues')
    
    # Comprehensive dashboard
    create_comprehensive_dashboard(df, word_freq, theme_analysis)
    
    # 5. Generate insights
    insights = generate_insights(df, word_freq, theme_analysis)
    
    # 6. Create insight report
    report_filename = create_insight_report(insights, df, word_freq, theme_analysis)
    
    # 7. Print results
    print("\n" + "=" * 60)
    print("ğŸ¯ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìš”ì•½")
    print("=" * 60)
    
    for i, insight in enumerate(insights, 1):
        print(f"\n{i}. {insight['category']}")
        print(f"   ğŸ’¡ {insight['finding']}")
        print(f"   ğŸ“ˆ {insight['implication']}")
        print(f"   ğŸ¬ {insight['action']}")
    
    print(f"\n" + "=" * 60)
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(f"â€¢ ë¶„ì„ëœ ë¦¬ë·° ìˆ˜: {len(df):,}ê°œ")
    print(f"â€¢ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {len(word_freq):,}ê°œ")
    print(f"â€¢ í‰ê·  ë¦¬ë·° ê¸¸ì´: {df['ë‚´ìš©'].str.len().mean():.0f}ì")
    
    print(f"\nğŸ”¥ TOP 10 í‚¤ì›Œë“œ:")
    for i, (word, freq) in enumerate(word_freq.most_common(10), 1):
        print(f"{i:2d}. {word}: {freq:,}íšŒ")
    
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print(f"â€¢ ì¸ì‚¬ì´íŠ¸ ë³´ê³ ì„œ: {report_filename}")
    print(f"â€¢ ì›Œë“œí´ë¼ìš°ë“œ: sonplan_*ì›Œë“œí´ë¼ìš°ë“œ*.png")
    print(f"â€¢ ì¢…í•© ëŒ€ì‹œë³´ë“œ: sonplan_comprehensive_dashboard*.png")
    
    # Save processed data
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    df.to_csv(f'sonplan_cleaned_reviews_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    keywords_df = pd.DataFrame([
        {'í‚¤ì›Œë“œ': word, 'ë¹ˆë„': freq, 'ìˆœìœ„': i+1}
        for i, (word, freq) in enumerate(word_freq.most_common(100))
    ])
    keywords_df.to_csv(f'sonplan_keywords_{timestamp}.csv', index=False, encoding='utf-8-sig')
    
    print(f"â€¢ ì •ì œëœ ë¦¬ë·° ë°ì´í„°: sonplan_cleaned_reviews_{timestamp}.csv")
    print(f"â€¢ í‚¤ì›Œë“œ ë°ì´í„°: sonplan_keywords_{timestamp}.csv")
    
    print("\nâœ¨ ë¶„ì„ ì™„ë£Œ!")

if __name__ == "__main__":
    main()